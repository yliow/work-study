%-*-latex-*-
%https://online.stat.psu.edu/stat414/lesson/5/5.3

\sectionthree{Independence}
\begin{python0}
from solutions import *; clear()
\end{python0}

Suppose $p : S \rightarrow [0,1]$ is a probability distribution function.
Two events $A$ and $B$ with $p(A)>0, p(B)>0$ are \defone{independent} if
\[
p(A \mid B) = p(A) 
\]
Recall that by definition
\[
p(A \mid B) = \frac{p(A \cap B)}{p(B)}
\]
Therefore if $A$ and $B$ are independent, then
\[
p(A) = p(A \mid B) = \frac{p(A \cap B)}{p(B)}
\]
Therefore
\[
  p(A)p(B) = p(A \cap B)
\]
Therefore the following are equivalent:
\[
  p(A \mid B) = p(A)
  \iff
  p(B \mid A) = p(B)
  \iff
  p(A \cap B) = p(A) \cdot p(B)
\]

By the way $p(A \cap B)$ is called the \defone{joint probability} of $A$ and $B$.

Intuitively, the fact that $A$ and $B$ are independent, i.e.,
\[
p(A \mid B) = p(A) 
\]
means that the chance of $A$ is not dependent on whether
$B$ has occurred or not.

Take for instance intuitively the probability of getting a one
when you roll a die knowing that you get a one or two or three or four
or five or six should be the same as getting a one.
However, the probability of getting a one if I get a one or two is defintely
higher than the probability of getting a one:
\begin{align*}
  p(\{\ONE\} \mid \{\ONE, \TWO\}) = \frac{p(\{\ONE\})}{p(\{\ONE, \TWO\})}
  = \frac{1}{2}
  \neq p(\{\ONE\})
\end{align*}
In this case, $B$ in $p(A \mid B)$ actually gives you more information.

You can also talk about the independence of two random variables:

\begin{defn}
  Let $X$ and $Y$ be random variables on sample space $S$:
  $X : S \rightarrow V$ and
  $Y : S \rightarrow V'$ are functions.
  We say that $X$ and $Y$ are \defone{independent} if
  \[
  \Pr[X=x \text{ and } Y=y] = \Pr[X=x] \cdot \Pr[Y=y]
  \]
  for all $x \in X(S)$ and $y \in Y(S)$.
  The above condition is the same as
  \[
    \Pr[X=x \mid Y = y] = \Pr[X=x]
  \]
  which is the same as
  \[
    \Pr[Y=y \mid X = x] = \Pr[Y=y]
  \]
  Note that this definition does \textit{not} depend on
  $X$ and $Y$ mapping to $\R$; in fact they don't even need to map to the same set.
  Note that
  $\Pr[X=x \text{ and } Y=y]$ means
  \[
    \Pr[X=x \text{ and } Y=y]
    = p(\{s\in S \mid X(s) = x, Y(s) = y \})
  \]
  I will also write
  $\Pr[X=x \text{, } Y=y]$ or
  $\Pr[(X=x) \land (Y=y)]$ for
  $\Pr[X=x \text{ and } Y=y]$.
\end{defn}

\input{discrete-probability/exercises/disc-prob-21/main.tex}

\begin{thm}
  Let $X$ and $Y$ be independent.
  Then
  \[
  \E[XY] = \E[X] \cdot \E[Y]
  \]
\end{thm}


\proof
Exercise.
\qed



\subsection{An experiment involving two experiments}

Let's consider the case of a random experiment that involves
performing \textit{two} random experiments, one after another.
Consider a random experiment $R$
that involves the tossing two fair coins, say I call the experiment of
rolling the first coin $R_1$ and the second $R_2$.
Suppose $p_1$ and $p_2$ be the pdf of the die 1 and die 2 respectively.
For simplicity, suppose both coin are fair.
I will denote the outcomes of $R$ by
\begin{align*}
S
&= \{\HEAD, \TAIL\}^2 \\
&= \{ (\HEAD,\HEAD), (\HEAD,\TAIL), (\TAIL,\HEAD), (\TAIL, \TAIL) \}
\end{align*}
Since the two coins are fair, each outcome is equally likely:
\[
p(x) = 1/4
\]
for $x \in S$.

Consider the statement: \lq\lq What is the probability of
getting a tail for the second coin if the first coin is a head''.
We have two events.
Let
\[
A = \{\text{outcomes where the second toss gives a tail}\}
\]
and
\[
B = \{\text{outcomes where the first toss gives a head}\}
\]
So 
\lq\lq the probability of
getting a tail for the second coin is the first coin is a head''
which might be informally written as
\[
p(\text{second coin = T} \mid \text{first coin = H})
\]

Formally, of course $A$ is just
\[
A = \{(\HEAD,\TAIL), (\TAIL, \TAIL)\}
\]
and
\[
B = \{(\HEAD,\HEAD), (\HEAD,\TAIL)\}
\]
Then
\[
p(A \mid B)
= \frac{p(A \cap B)}{p(B)}
= \frac{p(\{ (\HEAD, \TAIL) \})}{p(\{(\HEAD,\HEAD), (\HEAD,\TAIL)\})}
= \frac{1/4}{1/4 + 1/4} = 1/2
\]

Here's another example.
Suppose I roll a die and toss a coin.
One would expect the output of the die to be independent of the coin.
To be specific, the event of getting a getting a six on the die
to be independent of the event that we get a tail for the coin.
Let's verify that.
The sample space is
\[
S = \{ \HEAD, \TAIL \} \times \{\ONE, \TWO, ... \SIX \}
\]
We want to verify that $p(A) = p(A \mid B)$ where
$A = \{\TAIL\} \times \{\ONE, ..., \SIX\}$
and
$B = \{\HEAD, \TAIL\} \times \{\SIX\}$.
Then
\begin{align*}
  p(A \mid B)
  &= \frac{p(A \cap B)}{p(B)} \\
  &= \frac{p(\{(\TAIL, \SIX)\})}{p(\{(\HEAD,\SIX),(\TAIL,\SIX)\})} \\
  &= \frac{1/12}{2/12} \\
  &= 1/2
\end{align*}
and
\begin{align*}
  p(A)
  &= p(\{(\TAIL, \ONE), ..., (\TAIL, \SIX)\}) \\
  &= 6 \cdot \frac{1}{12} \\
  &= 1/2
\end{align*}

\input{discrete-probability/exercises/disc-prob-22/main.tex}
\input{discrete-probability/exercises/disc-prob-23/main.tex}

The concept of independence for two events can be extended to
a collection of any number of events.
For the case when there are more than two events,
there are two concepts of independence:

\begin{defn}
Let $A_1, A_2, \ldots, A_n$ be events.
\begin{enumerate}
\item $A_1, \ldots, A_n$ are 
\defone{pairwise independent} if
$A_i$ and $A_j$
independent for $i \neq j$, i.e.,
\[
p(A_i \cap A_j) = p(A_i) p(A_j)
\]
\item $A_1, \ldots, A_n$ are 
  \defone{mutually independent} if
  for any collection of the above $A_i$'s, say
  $A_{i_0}$,
  $A_{i_1}$, ...
  $A_{i_{k-1}}$
  where $i_0 < i_1 < \cdots < i_{k-1}$,
  we have  
  \[
  p(A_{i_0} \cap \cdots \cap A_{i_{k-1}}) = p(A_{i_0}) \cdots p(A_{i_{k-1}})
  \]
\end{enumerate}
\end{defn}
