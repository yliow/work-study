%-*-latex-*-
\sectionthree{Baye's Theorem}
\begin{python0}
from solutions import *; clear()
\end{python0}

Recall that earlier,
I talked about a random experiment that is
made up of performing two independent experiments.
Now suppose I have two random experiments which
are not independent.
For instance, I have two boxes with the following contents:
\begin{tightlist}
\li box 1: 1 green balls and 3 red balls
\li box 2: 2 green balls and 1 red balls
\end{tightlist}
The random experiment involve choosing one box and
then picking a ball from
the box.
Notice that the event of picking a red ball (the
second random experiment) depends on the first (the
first random experiment).

Recall the we define
\[
p(A \mid B) = \frac{p(A\cap B)}{p(B)}
\]
This is the probability of $A$ given $B$ has occured.
Note that we also have
\[
p(B \mid A) = \frac{p(B\cap A)}{p(A)}
\]
Therefore we get
\begin{align*}
p(A\mid B) \cdot p(B) &= p(A \cap B) \\
p(B\mid A) \cdot p(A) &= p(B \cap A) 
\end{align*}
Since $A\cap B = B \cap A$, we get the following immediately:
\[
p(B \mid  A) \cdot p(A)
=
p(A \mid  B) \cdot p(B) 
\]
Notice the beautiful symmetry?

Beauty aside ... what's the whole point of the above?
Well, sometimes you will find that given $A$ and $B$,
it's pretty easy to 
compute $p(A\mid B)$ instead of $p(B\mid A)$.

Let me give  you an example ...

\newpage
\begin{eg}
I have two boxes with the following contents:
\begin{tightlist}
\li box 1: 1 green balls and 3 red balls
\li box 2: 2 green balls and 1 red balls
\end{tightlist}
The random experiment involve choosing one box and then picking a ball from
the box.
The possible outcomes are:
\begin{tightlist}
  \li $(1, G)$
  \li $(1, R)$
  \li $(2, G)$
  \li $(2, R)$
\end{tightlist}
where the first coordinate represents the box chosen and the
second is the color of the ball chosen from the box indicated by the
first coordinate.
Now let me define two events:
\begin{align*}
B &= \text{I choose box 1 (and after that I pick a ball from box 1)} \\
A &= \text{I pick a red ball (after I picked a box)}
\end{align*}
Now let's think about $p(B\mid A)$ and $p(A\mid B)$.
\end{eg}

$p(A\mid B)$ is easy: this is the probability of choosing a red ball if 
I picked box 1.
Why is this easy? Well, if I picked box 1,
then the probability of picking a red
ball must be $3/4$ since there are 3 red balls out of altogether 4 
balls in box 1.
\[
p(A \mid B) = \frac{3}{4}
\]
Easy!
Formally
$A = \{(1, R), (2, R)\}$
and $B = \{(1, R), (1, G)\}$.
Therefore
\[
p(A \mid B) = \frac{p(\{(1, R)\})}{p(\{(1, R), (1, G)\})}
= \frac{1/2 \cdot 3/4}{1/2 \cdot 3/4 + 1/2 \cdot 1/4}
= \frac{3}{4}
\]


%[Include section on experiment which is a sequence of
%experiments which are dependent.
%\[
%p(B \mid A)
%= \frac{p(\{(1, R)\})}{p(\{(1, R), (2, R)\})}
%= \frac{1/2 \cdot 3/4}{1/2 \cdot 3/4 + 1/2 \cdot 1/3}
%= \frac{3/4}{3/4 + 1/3}
%\]
%]


However it's not immediately clear how to compute $p(B\mid A)$.
This is the probability of choosing box 1 if I picked a red ball.
The earlier result:
\[
p(B \mid  A) \cdot p(A)
=
p(A \mid  B) \cdot p(B) 
\]
tells us that there's a connnection between $p(A\mid B)$ and $p(B\mid A)$.
Of course I just said that $p(A\mid B)$ is $3/4$.
Furthermore $p(B)$ is 1/2.
The above becomes:
\[
p(B \mid  A) p(A)
= 
\frac{3}{4} \cdot \frac{1}{2} = \frac{3}{8}
\]
I'm done if I can figure out $p(A)$, since from the above I have
\[
p(B \mid  A)
= 
\frac{1}{p(A)} \cdot \frac{3}{8}
\]
$p(A)$ is the probability of choosing a red ball,
This is rather complicated since there are two separate scenarios:
a red ball from box 1 or a red ball from box 2.
Now what?

Well ... I just have to consider the two separate scenarios! Duh!

I have to consider the scenario where the red ball is taken from box 1
and then the scenario where the red ball is taken from box 2.
Note that the two scenario are disjoint.
Mathematically, I'm simply computing $p(A)$ 
by doing this (informally speaking):
\[
p(A) = p(\text{(box 1 case)} \cap A) + p(\text{(box 2 case)} \cap A)
\] 
Using $B$, this is just
\[
p(A) = p(B \cap A) + p(\overline B \cap A) 
\] 
Note that the two events 
$B \cap A$ and 
$\overline B \cap A$
are disjoint and the union is indeed $A$.
In other words I'm using the general fact that if events 
$E_1$ and $E_2$ are disjoint, then
\[
p(E_1 \cup E_2) 
= p(E_1) + p(E_2) - p(E_1 \cap E_2) 
= p(E_1) + p(E_2) - 0
= p(E_1) + p(E_2)
\]

Now let's compute $p(A \cap B)$.
Note this is the probability that I picked box 1 
\textit{and} I picked a red ball.
This is \textit{not} the probability that I picked a red ball
\textit{given} that I picked box 1.
\[
p(A \cap B) = p(A\mid B) p(B) = \frac{3}{4} \cdot \frac{1}{2} = \frac{3}{8}
\]
The other quantity is
\[
p(A \cap \overline B) 
= p(A\mid \overline B) p(\overline B) 
= \frac{1}{3} \cdot \frac{1}{2} = \frac{1}{6}
\]
Putting things together we get
\[
p(A) = p(B \cap A) + p(\overline B \cap A) = \frac{3}{8} + \frac{1}{6}
= \frac{13}{24} 
\] 

Putting everything together we get
\[
p(B \mid  A)
= \frac{1}{p(A)} \cdot \frac{3}{8}
= \frac{24}{13} \cdot \frac{3}{8}
= \frac{9}{13}
\]

Let me finish by saying that sometimes if you want to compute 
$p(B\mid A)$, it might be easier to use this
\[
p(B \mid A) \cdot p(A) 
= 
p(A \mid B) \cdot p(B)
\]
assuming that it's easier to compute $p(A \mid B)$.
And for the computation of $p(A)$, in some cases,
it might be easier to use
\[
p(A) = p(A \cap B) + p(A \cap \overline B)
\]
where the quantities on the right can be computed using
\begin{align*}
p(A \cap B) &= p(A \mid B) \cdot p(B) \\
p(A \cap \overline B) &= p(A \mid \overline B) \cdot p(\overline B)
\end{align*}

If you put everything into the expression for $p(B \mid A)$, you get this
\[
p(B \mid A)
=
\frac{1}{p(A \mid B) p(B) + p(A \mid \overline B) p(\overline B)}
\cdot 
p(A \mid B) \cdot p(B)
\]

Before I state the computational technique as a theorem 
(the Baye's Theorem) let me compute the same probability
the naive way, staight from definition by listing the 
outcomes.

In that case the sample space of our experiment is:
\begin{Verbatim}[fontsize=\small, frame=single]
1, G         B
1, R    A    B
1, R    A    B
1, R    A    B   
2, G
2, G
2, R    A
\end{Verbatim}
(The first column is the box, the second is the ball. The A and B
indicates which event the row belongs to.)
However, note that the number of items is not correct.
For instance note that probability of choosing box 1 must be 0.5.
So let's be duplicate the first 4 lines 3 times and the last 3 line 4 times:
\begin{Verbatim}[fontsize=\small, frame=single]
1, G         B
1, R    A    B
1, R    A    B
1, R    A    B   
1, G         B
1, R    A    B
1, R    A    B
1, R    A    B   
1, G         B
1, R    A    B
1, R    A    B
1, R    A    B   
2, G
2, G
2, R    A
2, G
2, G
2, R    A
2, G
2, G
2, R    A
2, G
2, G
2, R    A
\end{Verbatim}
The probability we want is $p(B \mid A)$ which is defined to be
$p(B \cap A)/p(A)$.
Now, from the above table, $p(B \cap A) = 9/24 = 3/8$.
Also, $p(A) = 13/24$.
Therefore $p(B \mid A) = (3/8)/(13/24) = 9/13$.

Our brute force computation does match our computation above.

Now I'm going to give you a \textit{third} method using a simulation.
This is actually a random experiment to get an approximation
of the probability that we want.
Because it's a simulation, it won't be exact.
However the more random trials we simulate the closer we get
to the actual probability.
The code below explains itself.
I'm going to simulate 1000000 random trials of the given experiment.
\begin{Verbatim}[frame=single, fontsize=\small]
import random;random.seed()
count1 = 0 # count of outcomes in A and B
count2 = 0 # count of outcomes in A

N = 10000000

for i in range(N):  
    # randomly pick box 1 or box 2
    box = random.randrange(1, 3)
    # randomly pick ball as 'G' or 'R'
    # based on which box was chosen
    if box == 1:
        ball = random.choice(['G', 'R', 'R', 'R'])
    else:
        ball = random.choice(['G', 'G', 'R'])
    if ball == 'R': # outcome is in A
        count2 += 1
        if box == 1: # outcome is in B
            count1 += 1

    #if i % 100000 == 0 and count2 != 0:
print(float(count1) / count2)
\end{Verbatim}
The output is 
\begin{Verbatim}[frame=single, fontsize=\small]
0.692257273456
\end{Verbatim}
and 
\[
\frac{9}{13} = 0.6923076923076923...
\]
so the simulation agrees with our exact computation by up to 3 decimal
places.

\newpage

\begin{ex}
I toss a fair coin.
If the outcome is head, I toss an unfair coin with a probability of getting
a head being 1/3.
If the outcome of the first coin is tail, 
I toss another unfair coin with a probability of getting a head being 1/5.
What is the probability that the first outcome
is a head if the second outcome is a tail?
\end{ex}

Let $A$ be the event that the first outcome is a head
and $B$ be the event that the second outcome is a tail.
We are given:
\begin{align*}
p(A) &= p(\overline A) = \frac{1}{2} \\
p(B \mid A) &= \frac{2}{3} \\
p(B \mid \overline A) &= \frac{4}{5}
\end{align*}
We want $p(A \mid B)$.
We have
\[
p(A \mid B) \cdot p(B) = p(B \mid A) \cdot p(A)
\]
which implies
\begin{align*}
p(A \mid B) 
&= \frac{1}{p(B)} \cdot p(B \mid A) \cdot p(A) \\
&= \frac{1}{p(B)} \cdot \frac{2}{3} \cdot \frac{1}{2} \\
&= \frac{1}{p(B)} \cdot \frac{1}{3}
\end{align*}
Now
\begin{align*}
p(B) 
&= p(B \cap A) + p(B \cap \overline A) \\
&= p(B \mid A) \cdot p(A) + p(B \mid \overline A) \cdot p(\overline A) \\
&= \frac{2}{3} \cdot \frac{1}{2} 
   + 
   \frac{4}{5} \cdot \frac{1}{2} \\
&= \frac{1}{3} + \frac{2}{5} \\
&= \frac{11}{15}
\end{align*}
Therefore
\begin{align*}
p(A \mid B) 
= \frac{1}{p(B)} \cdot \frac{1}{3} 
= \frac{15}{11} \cdot \frac{1}{3}
=\frac{5}{11}
\end{align*}
\qed


\newpage
\newcommand\BALLONE{\text{\textsc{Ball1}}}
\newcommand\BALLTWO{\text{\textsc{Ball2}}}
\begin{ex}
I have three boxes. 
\begin{tightlist}
\item Box 1: 3 red balls, 2 white balls
\item Box 2: 4 red balls, 2 white balls
\item Box 3: 2 red balls, 3 white balls
\end{tightlist}
This is what I'm going to do. 
I first draw a ball from Box 1.
If the ball from Box 1 is red, I pick a ball from Box 2.
If the ball from Box 1 is white, I pick a ball from Box 3.

For this problem,
I will deliberately use random variable notation instead of
events notation.
I will leave it to you to define the random variable formally.
Just from the name of the random variable, you should see quickly how to 
define the random variable formally and correctly.

Here are the questions:
\begin{tightlist}
\item What is $\Pr[\BALLTWO = \RED \mid \BALLONE = \RED]$?
\item What is $\Pr[\BALLONE = \RED \mid \BALLTWO = \RED]$?
\end{tightlist}
\end{ex}


The first probability is easy:
\[
\Pr[\BALLTWO = \RED \mid \BALLONE = \RED] = \frac{4}{6} = \frac{2}{3}
\]
For the second probability we use Baye's theorem:
\begin{align*}
&\Pr[\BALLONE = \RED \mid \BALLTWO = \RED] \cdot \Pr[\BALLTWO = \RED]  \\
&\hspace{1cm} = \Pr[\BALLTWO = \RED \mid \BALLONE = \RED] \cdot \Pr[\BALLONE = \RED] 
\end{align*}
which gives us the following immediately:
\begin{align*}
\Pr[\BALLONE = \RED \mid \BALLTWO = \RED] \cdot \Pr[\BALLTWO = \RED]
= \frac{2}{3} \cdot \frac{3}{5} = \frac{2}{5}
\end{align*}
and therefore
\[
\Pr[\BALLONE = \RED \mid \BALLTWO = \RED]  
= 
\frac{1}{\Pr[\BALLTWO = \RED]} \cdot \frac{2}{5}
\]
Now
\begin{align*} 
  &\Pr[\BALLTWO = \RED] \\
  &\hspace{0.5cm} =
    \Pr[\BALLTWO=\RED,  \BALLONE=\RED] 
    + 
    \Pr[\BALLTWO=\RED,  \BALLONE \neq \RED] \\
  &\hspace{0.5cm} =
    \Pr[\BALLTWO=\RED \mid  \BALLONE=\RED] \cdot \Pr[\BALLONE=\RED] \\
  &\hspace{1cm} + 
    \Pr[\BALLTWO=\RED \mid  \BALLONE \neq \RED] \cdot \Pr[\BALLONE \neq \RED] \\
  &\hspace{0.5cm} =
    \frac{4}{6} \cdot \frac{3}{5} +
    \frac{2}{5} \cdot \frac{2}{5} \\
  &\hspace{0.5cm} =
    \frac{2}{5} +
    \frac{4}{25} \\
  &\hspace{0.5cm} = \frac{14}{25}
\end{align*}
Therefore
\begin{align*}
\Pr [ \BALLONE = \RED \mid \BALLTWO = \RED ]
&= \frac{1}{\Pr[\BALLTWO = \RED]} \cdot \frac{2}{5} \\
&= \frac{1}{14/25} \cdot \frac{2}{5} \\
&= \frac{25}{14} \cdot \frac{2}{5} \\ 
&= \frac{5}{7} 
\end{align*}

Here's a simulation:
\begin{Verbatim}[frame=single, fontsize=\small]
import random;random.seed()
count1 = 0 # count of ball 1 = 'R'
count2 = 0 # count of ball 2 = 'R'

for i in xrange(10000000): 
 
    ball1 = random.choice(['R','R','R','W','W'])

    if ball1 == 'R' :
        ball2 = random.choice(['R','R','R','R','W','W'])
    else:
        ball2 = random.choice(['R','R','W','W','W'])
        
    if ball2 == 'R':
        count2 += 1
        if ball1 == 'R':
            count1 += 1

print(float(count1) / count2)
\end{Verbatim}
The output is 
\begin{Verbatim}[frame=single, fontsize=\small]
0.714076007643
\end{Verbatim}
which agrees with our computation up to 3 decimal places:
\[
\frac{5}{7} = 0.7142857142857143...
\]
\qed

\newpage

Note that the first question is easy:
Given that \BALLONE\ is red, the ball we draw would be from Box 2 and the 
probability of drawing a red is $\frac{4}{6} = \frac{2}{3}$.

Now for our theorem ...

\newpage
\begin{thm} \textsc{(Bayes' Theorem)}
\[
p(B \mid A) 
=  
\frac{1}{p(A \mid  B) \cdot p(B) + p(A \mid  \overline B) \cdot p(\overline B)}
\cdot 
p(A \mid  B) \cdot p(B)
\]
\end{thm}

Let's prove this theorem.
Recall that
\[
p(B \mid A) \cdot p(A) = p(A \mid B) \cdot p(B)
\]
Therefore
\[
p(B \mid A) = \frac{1}{p(A)} \cdot p(A \mid B) \cdot p(B)
\]
Note that we just need to prove
\[
p(A) = p(A \mid  B) \cdot p(B) + p(A \mid  \overline B) \cdot p(\overline B)
\]
The right hand side is
\begin{align*}
p(A \mid  B) p(B) + p(A \mid  \overline B)p(\overline B) 
&= \frac{p(A \cap B)}{p(B)} p(B) 
   + \frac{p(A \cap \overline{B})}{p(\overline B)} 
   p(\overline B) \\
&= p(A \cap B) + p(A \cap \overline{B}) 
\end{align*}
The two events $A \cap B$ and $A \cap \overline B$ are clearly
disjoint, and therefore
\[
p(A \cap B) + p(A \cap \overline{B}) = p(A)
\]
and we're done!
\qed



QUESTION:
Which is better
\[
p(B \mid A) 
=  
\frac{1}{p(A \mid  B) \cdot p(B) + p(A \mid  \overline B) \cdot p(\overline B)}
\cdot 
p(A \mid  B) \cdot p(B)
\]
or
\[
p(B \mid A) 
=  
\frac{1}{p(A \cap B) + p(A \cap \overline{B})}
\cdot 
p(A \mid  B) \cdot p(B)
\]
In other words this is better:
\[
p(A \mid  B) \cdot p(B) + p(A \mid  \overline B) \cdot p(\overline B)
\]
or
\[
p(A \cap B) + p(A \cap \overline{B})
\]

Bayes' theorem can be generalized in the following way.
The basic Bayes' theorem say this:
\[
p(B \mid A) 
=  
\frac{1}{p(A \mid  B) \cdot p(B) + p(A \mid  \overline B) \cdot p(\overline B)}
\cdot 
p(A \mid  B) \cdot p(B)
\]
This comes from
\[
p(B \mid A) p(A) = 
p(A \mid B) p(B) 
\]
which gives us
\[
p(B \mid A)  =
\frac{1}{p(A)} \cdot
p(A \mid B) p(B) 
\]
The term $p(A)$ is then computed using
\[
p(A) = p(A \cap B) + p(A \cap \overline{B}) 
\]
Hence it's easy to see that if instead of $B$ and $\overline B$,
I have a collection of events $B_0, ..., B_{n-1}$ such that
$B_i \cap B_j = \emptyset$ for $i \neq j$ and
$\cup_{i=0}^{n-1}B_i = S$, then
\[
p(B_i \mid A) p(A) =
\frac{1}{p(A)} \cdot
p(A \mid B_i) p(B_i) 
\]
and
\[
p(A) = \sum_{i=0}^{n-1} p(A \cap B_i)  
\]
Putting all the above together, I get
\[
p(B_i \mid A)  =
\frac{1}{\sum_{i=0}^{n-1} p(A \cap B_i)} \cdot
p(A \mid B_i) p(B_i) 
\]
Now from
\[
p(A \mid B_i) = \frac{p(A \cap B_i)}{p(B_i)} 
\]
I get
\[
p(A \mid B_i) p(B_i) =  p(A \cap B_i) 
\]
Therefore
\[
p(B_i \mid A)
=
\frac{1}{\sum_{i=0}^{n-1} p(A \cap B_i)} \cdot
p(A \mid B_i) p(B_i)
=
\frac{1}{\sum_{i=0}^{n-1} p(A \mid B_i) p(B_i)} \cdot
p(A \mid B_i) p(B_i)
\]
