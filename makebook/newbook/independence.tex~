\section{Independence}

Suppose $p : S \rightarrow [0,1]$ is a probability distribution function.
Two events $A$ and $B$ are \defterm{independent} iff
\[
p(A \mid B) = p(A) 
\]
Recall that by definition
\[
p(A \mid B) = \frac{p(A \cap B)}{p(B)}
\]
Therefore if $A$ and $B$ are independent, then
\[
p(A) = p(A \mid B) = \frac{p(A \cap B)}{p(B)}
\]
Therefore
\[
p(A)p(B) = p(A \cap B)
\]

Intuitively, the fact that $A$ and $B$ are independent, i.e.,
\[
p(A \mid B) = p(A) 
\]
means that the chance of $A$ is not not depedent on whether
$B$ has occurred or not.

Take for instance intuitively the probability of getting a one
when you roll a die knowing that you get a one or two or three or four
or five or six should be the same as getting a one.
However, the probability of getting a one if I get a one or two is defintely
higher than the probability of getting a one:
\begin{align*}
  p(\{\ONE\} \mid \{\ONE, \TWO\}) = \frac{p(\{\ONE\})}{p(\{\ONE, \TWO\})}
  = \frac{1}{2}
  \neq p(\{\ONE\})
\end{align*}

\subsection{An experiment involving two experiments}

Let's consider the case of a random experiment that involves
performing \textit{two} random experiments, one after another.
Consider a random experiment, call it $R$ for convenience,
that involves the rolling two fair coins, say I call the experiment of
rolling the first coin $R_1$ and the second $R_2$.
Suppose $p_1$ and $p_2$ be the pdf of the die 1 and die 2 respectively.
For simplicity, suppose both coin are fair.
I will denote the outcomes of $R$ by
\[
S = \{ HH, HT, TH, TT \}
\]
Since the two coins are fair, each outcome is equally likely:
\[
p(x) = 1/4
\]
for $x \in S$.

Consider the statement: \lq\lq What is the probability of
getting a tail for the second coin if the first coin is a head''.
We have two events.
Let
\[
A = \{\text{outcomes where the second coin is a tail}\}
\]
and
\[
B = \{\text{outcomes where the first coin is a head}\}
\]
So 
\lq\lq the probability of
getting a tail for the second coin is the first coin is a head''
which is might be informally as
\[
p(\text{second coin = T} \mid \text{first coin = H})
\]

Note that 
\[
A = \{HT, TT\}
\]
and
\[
B = \{HH, HT\}
\]
Then
\[
p(A \mid B)
= \frac{p(A \cap B)}{p(B)}
= \frac{p(\{HT\})}{p(\{HH, HT\})}
= \frac{1/4}{1/4 + 1/4} = 1/2
\]

\begin{defn}
Let $A_1, A_2, \ldots, A_n$ be events.
\begin{enumerate}
\item $A_1, \ldots, A_n$ are 
\defterm{pairwise independent} if
\[
A_i \text{ and } A_j
\]
are independent for $i \neq j$.
\item $A_1, \ldots, A_n$ are 
\defterm{mutually independent} if
\[
p(A_1 \cap \cdots \cap A_n) = p(A_1) \cdots p(A_n) 
\]
\end{enumerate}
\end{defn}


Here's another example.
Suppose I roll a die and toss a coin.
One would expect the output of the die to be independent of the coin.
To be specific, the event of getting a getting a six on the die
to be independent of the event that we get a tail for the coin.
Let's verify that.
The sample space is
\[
S = \{ \HEAD, \TAIL \} \times \{\ONE, \TWO, ... \SIX \}
\]
We want to verify that $p(A) = p(A \mid B)$ where
$A = \{\TAIL\} \times \{\ONE, ..., \SIX\}$
and
$B = \{\HEAD, \TAIL\} \times \{\SIX\}$.
Then
\begin{align*}
  p(A \mid B)
  &= \frac{p(A \cap B)}{p(B)} \\
  &= \frac{p(\{(\TAIL, \SIX)\})}{p(\{(\HEAD,\SIX),(\TAIL,\SIX)\})} \\
  &= \frac{1/12}{2/12} \\
  &= 1/2
\end{align*}
and
\begin{align*}
  p(A)
  &= p(\{(\TAIL, \ONE), ..., (\TAIL, \SIX)\}) \\
  &= 6 \cdot \frac{1}{12} \\
  &= 1/2
\end{align*}

