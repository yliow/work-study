\section{Discrete Probability}

[See also probability section in cryptography.]

\newcommand\op[1]{{\operatorname{#1}}}
\newcommand\outcome[1]{{\operatorname{\text{\textsc{#1}}}}}

\newcommand\cpp{\operatorname{C++}}
\newcommand\py{\operatorname{PYTHON}}
\newcommand\buggy{\operatorname{BUGGY}}

\section{Probabilistic Experiments}

If you have a coin, you can toss it and get either head or tail.
(Don't be facetious ... it won't land on its side ... unless if you have
an extremely \textit{ thick} coin ...)
If the coin is fair, half the time you will get a head and 
half the time you will get a tail.
Let's call each toss a random experiment and let's
call the result of your toss the outcome of the experiment.

If you're the gamling kind, you know that in fact there's a slightly
higher change that the head falls face down so that 
there's a slightly higher chance of getting the tail.
Let's ignore this fact and just assume that our coin is absolutely fair.

If you don't have a fair coin, you can write a 
program simulate the tossing of the coin like this one
in your favorite programming language (C/C++, Java, Python, what-have-you):
\includesourcenonumbers{prob/tossfaircoin1.py}

Here's my output when I run the program:
\bShell[python prob/tossfaircoin1.py >abc.output 2>abc.output]
\eShell
{\footnotesize
\includesourcenonumbers{abc.output}
}

Much better isn't it?
Not because I don't have a quarter, but because I can easily
do a million coin-toss experiments in a split second by changing \verb!N!.
I can also collect all the outcomes and tabulate:
{\small
\includesourcenonumbers{prob/tossfaircoin2.py}
}

Here's my output when I run the program:
\bShell[python prob/tossfaircoin2.py >abc.output 2>abc.output]
\eShell
{\footnotesize
\includesourcenonumbers{abc.output}
}

\begin{ex}
Run the above program with a very large \verb!N! (say 1000000).
What is the probability of getting a head? Tail?
No surprises, right?
\qed
\end{ex}

Of course if your \verb!N! is really, really, really huge,
you will find that the probability of getting a head is 0.5 and 
the probability of getting a tail is 0.5.

Here's a function (derived by simulation) 
that gives us the probability for each possible outcome
of our experiment:
\includesourcenonumbers{prob/tossfaircoin3.py}
I've increase \verb!N! to 1000 and 
also commented out the printing of each experiment.
Here's my output when I run the program:
\bShell[python prob/tossfaircoin3.py >abc.output 2>abc.output]
\eShell
{\footnotesize
\includesourcenonumbers{abc.output}
}

Of course if we assume from the beginning that our coin is fair we can 
save the trouble of the computation:
\includesourcenonumbers{prob/tossfaircoin4.py}
Here's my output when I run the program:
\bShell[python prob/tossfaircoin4.py >abc.output 2>abc.output]
\eShell
{\footnotesize
\includesourcenonumbers{abc.output}
}
Of course when \verb!N! gets larger and larger, the probability function
derived using simulations will match this new \lq\lq theoretically'' derived
function.

\begin{ex}
What if I change the \verb!toss_coin! function to this:
\begin{Verbatim}[frame=single]
def toss_coin():
    r = random.randrange(5)
    if r <= 1: 
        return TAIL
    else:
        return HEAD
\end{Verbatim}
What is the probability of getting a head by simulations? Tail?
What are the theoretical values?
\qed
\end{ex}


\begin{ex}
Make a copy of the above program and then modify it to roll
a fair die.
Make 1000 rolls and report the number for each outcome.
What is the simulated probability function?
What is the theoretical probability function?
\qed
\end{ex}


\begin{ex}
Write a program to simulate throwing two dice and returning the sum of their
values.
Of course the outcomes are 2, 3, 4, ..., 12.
Are the probabilities for these outcomes the same?
This is pretty helpful if you're gambling right?
\qed
\end{ex}



\newpage
\section{Experimental (or Empirical) Approach}
Here's how to think of probability intuitively:
You can of course think of probability as some kind of averaging.
In the case of tossing a particular coin, if you say
\[
p(\HEAD) = \frac{1}{3}
\]
what you meant is that if you toss the coin 3 times, then
it's likely that 
approximately 1 out of the 3 is a head;
if you toss the coin 300 times, then it's very likely that
approximately 100 of the 300 are heads;
if you toss the coin 3000 times, then it's very very likely that
approximately 1000 of the 3000 are heads; etc.
The more tosses you make, the closer you get to \lq\lq one third of the
tosses comes out head''.




\newpage
\section{Probability Distribution Function}

A (finite) discrete 
\defterm{probability distribution function} is a function 
\[
p : S \rightarrow \R
\]
where $S$ is a finite set such that 
\begin{axioms}
\item[{[DP-1]}] $0 \leq p(x) \leq 1$ for all $x \in S$
\item[{[DP-2]}] $\sum_{x \in S} p(x) = 1$
\end{axioms}

Because of [DP-1], I can also write $p$ like this:
\[
p : S \rightarrow [0,1]
\]
For simplicity, if $A$ is a subset of $S$, I'll write
\[
p(A) = \sum_{x \in A} p(x)
\]
[DP-2] is the same as saying
\[
p(S) = 1
\]
In the above context, $S$ is a \defterm{sample space},
the elements of $S$ are called \defterm{outcomes},
and a subset of $S$ is called an \defterm{event}.

Here are some basic facts about probability functions.

\begin{thm} Let $p : S \rightarrow [0,1]$ be 
a probability distribution function.
\begin{tightlist}
\item If $A \subseteq S$, then $0 \leq p(A) \leq 1$.
\item $p(\emptyset) = 0$.
\item If $A$ and $B$ are subsets of $S$, then
\[
p(A \cup B) = p(A) + p(B) - p(A \cap B)
\]
\item If $A$ and $B$ are subsets of $S$, then
\[
p(A \cup B) \leq p(A) + p(B)
\]
\item If $A$ and $B$ are disjoint subsets of $S$, then
\[
p(A \cup B) = p(A) + p(B)
\]
\item If $A \subseteq S$, then $p(\overline{A}) = 1 - p(A)$.
\end{tightlist}
\end{thm}

\begin{ex}
Prove all the statements in the above theorem.
\qed
\end{ex}
    
\begin{eg}
For instance let's look at the result of tossing a coin.
In this case, \lq\lq result'' means \lq\lq the side of the coin facing up''
after the coin stops bouncing and spinning on the table.
This means that the result can be 
\[
\HEAD
\]
or
\[
\TAIL
\]
So we let 
\[
S = \{\HEAD, \TAIL\}
\]
\qed
\end{eg}

(Of course if you want to be facetious, you can say 
\lq\lq Hey ... what if the coin lands on its side?''
Well ... it's a matter of what you're trying to model.
If you do have a thick coin and you do want to account for that
(ridiculous) case, you just have a create another probability
function with a different sample space.)

\begin{eg}
Suppose that I've rigged my coin so that the chance of getting
a head is twice of getting the tail.
This means that the probability function
\[
p : S \rightarrow [0,1]
\]
is
\[
p(\HEAD) = \frac{2}{3}, \,\,\,\,\,
p(\TAIL) = \frac{1}{3} \,\,\,\,\,
\]
Of course if the coin is a fair coin I would have
\[
p(\HEAD) = \frac{1}{2} = p(\TAIL)
\]
\qed
\end{eg}


\begin{defn}
A probability distribution function $p : S \rightarrow [0,1]$ 
is said to be \defterm{uniform} if
\[
p(x) = \frac{1}{|S|}
\]
for each $x \in S$.
$|S|$, the size of $S$, denotes the numbers of distinct elements in $S$.
\end{defn}


\begin{eg}
Suppose I have a loaded die such that getting a \lq\lq 1'' 
is 10 times more likely 
than the others and the others are equally likely.
Let $p$ be the probability function of rolling this die.
Then
\begin{align*}
1 
&= p(\ONE) + p(\TWO) + \cdots + p(\SIX) \\
&= p(\ONE) + 5 \cdot p(\TWO) \\
&= p(\ONE) + 5 \cdot \frac{1}{10} p(\ONE) \\
&= \frac{3}{2} \cdot p(\ONE) 
\end{align*}
So 
\[
p(\ONE) = \frac{2}{3}
\]
and 
\[
p(\TWO) = p(\THREE) = ... = p(\SIX) = \frac{2}{30}
\]
And of course
\[
S = \{\ONE, \TWO, \THREE, \FOUR, \FIVE, \SIX\}
\]
Of course I'm using \lq\lq$\ONE$'' (in $S$) to symbolically represent 
\lq\lq the outcome where the face with one dot is the top face after a roll'', etc.

Go ahead and write a program to simulate tossing my loaded die.
Run it for a large number of experiments and check that you get the
expected results. 
\begin{Verbatim}[frame=single]
import random; random.seed()

ONE = "ONE"
TWO = "TWO"
THREE = "THREE"
FOUR = "FOUR"
FIVE = "FIVE"
SIX = "SIX"

# write a probability function for this experiment

def roll_die():
    r = random.randrange(30)
    if r < 20: return ONE
    elif r < 22: return TWO
    elif r < 24: return THREE
    elif r < 26: return FOUR
    elif r < 28: return FIVE
    else: return SIX

NUM_EXPERIMENTS = 20
for i in range(NUM_EXPERIMENTS):
    print "experiment", i, "... outcome:", roll_die()
\end{Verbatim}
\end{eg}


The following function can be used to produce the different
experiment functions (you should type it up and run it ... if you're
too lazy to do that you can also grab the code from
our class web site ... look for \lq\lq probability library"):
\begin{Verbatim}[frame=single]
def build_experiment(xs):
    total = sum([b for (a,b) in xs])
    accumulate = xs[0]
    for (a,b) in xs[1:]:
        accumulate.append((a, accumulate[-1][1] + b)
    def experiment():
        r = random.randrange(total)
        for (a,b) in accumulate:
            if r < b: return a
        raise ValueError("r:%s greater than total:%s" %\
                         (r, total))
    return experiment

HEAD = "HEAD"
TAIL = "TAIL"
toss_coin = build_experiment([(HEAD, 2),
                              (TAIL, 1),
                             ])
ONE = "ONE"
TWO = "TWO"
THREE = "THREE"
FOUR = "FOUR"
FIVE = "FIVE"
SIX = "SIX"
roll_die = build_experiment([(ONE, 20),
                             (TWO, 2),
                             (THREE, 2),
                             (FOUR, 2),
                             (FIVE, 2),
                             (SIX, 2),
                            ])
\end{Verbatim}


\newpage
\begin{ex}
The tetris game has 5 shapes: 
SQUARE, l, Z, S, T.
To make your tetrix game annoying, you have decided to make Z
5 times more likely to appear than S, S 5 times more likely than T,
T 5 times more like than l, and l 5 times more likely than SQUARE.
(And you make the shapes fall down real fast too.)
Write down the probabilities of the shapes. 
You gave your game to Dr. Liow who (promptly) dies after
playing the game for 20 seconds.
If a total of 100 shapes appear, approximately how many of each shape
appear.
\end{ex}


\newpage
\begin{ex}
The probability of getting a quiz from Dr. Liow during a regular
class meeting is 0.8.
What is the probability of getting 3 quizzes in a row?
Note that what is given is the experiment 
\lq\lq Attend a regular class meeting''.
(analogous to \lq\lq toss a coin'')
The sample space is
\[
S = \{\mathsc{Quiz}, \mathsc{NoQuiz}\}
\]
(analogous to $S= \{\HEAD, \TAIL$\}.)
Let's shorten that to 
\[
S = \{\mathsc{Q}, \mathsc{N}\}
\]
\end{ex}

Now what is asked is actually related to another
experiment: 
\lq\lq Attend 3 classes in a row''.
(Later you'll see how to view this as a \lq\lq product of
experiments''.)
There are two ways to compute an approximation of the desired
probability.

FIRST METHOD:
You should use the first experiment to generate a sequence of
$\mathsc{Q}$'s and
$\mathsc{N}$'s:
\[
\mathsc{Q},
\mathsc{Q},
\mathsc{N},
\mathsc{Q},
\mathsc{Q},
\mathsc{Q},
\mathsc{Q},
\mathsc{Q},
\mathsc{Q},
\mathsc{N},
\mathsc{Q},
\mathsc{Q},
...
\]
satisfying the given probability that
$p(\mathsc{Q}) = 0.8$ and
$p(\mathsc{N}) = 0.2$.
Next you look at all the consecutive triples.
For instance the above gives
\[
\mathsc{QQN},
\mathsc{QNQ},
\mathsc{NQQ},
\mathsc{QQQ},
\mathsc{QQQ},
\mathsc{QQQ},
\mathsc{QQQ},
\mathsc{QQQ},
\mathsc{QQN},
\mathsc{QNQ},
\mathsc{NQQ}, ...
\]
and then compute the approximate probability of seeing
$\mathsc{QQQ}$ in 
this sampling.
Of course you need to have a reasonbly huge sample.
You can either do this by hand or write a program to simulate the
experiment.

SECOND METHOD:
If you have a function for the first experiment, say
\verb!attend_class! which gives you $\mathsc{Q}$ or
$\mathsc{N}$, then you should have a
function say \verb!attend_3_classes! that uses \verb!attend_class!:
\begin{Verbatim}[frame=single]
Q = 'Q'
N = 'N'

def attend_3_classes():
    return [attend_class(), 
            attend_class(), 
            attend_class(),
           ]
\end{Verbatim}
Of course you can now generate samples and count:
\begin{Verbatim}[frame=single]
MAX = 100000
count = 0
for i in range(MAX):
    if attend_3_classes() == [Q, Q, Q]: 
        count += 1
print float(count) / MAX
\end{Verbatim}



\newpage
\begin{ex}
John Cantcode is a pretty bad programmer. 
At Fullabugz, it is known that the chance of him writing a buggy
C++ program is 3 times that of his Python program.
(Fullabugz uses only two languages. Why?
More than two means more headaches for John's manager.)
One third of the programs at Fullabugz is C++ programs.
His manager just ran a program and it crashed.
What is the probability that this was a C++ program?
(Do this empirically, i.e., do some simulations.)
Is there enough information to compute the probability?
If not, what is lacking?
\end{ex}




