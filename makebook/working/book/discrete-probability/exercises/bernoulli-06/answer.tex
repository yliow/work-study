%-*-latex-*-
\tinysidebar{\debug{exercises/discrete-probability/{bernoulli-06/answer.tex}}}

{\small
\begin{console}
import random; random.seed()

def get_flips(n, k):
    """
    Probability of getting a head is 1.0 / n
    Returns the number of flips of coin to reach k heads.
    """
    flips = 0
    heads = 0
    while heads < k:
        flips += 1
        face = random.randrange(n)
        if face == 0:
            heads += 1
    return flips

n = input("n where p(HEAD) is 1/n: ")
k = ("number of heads to reach: ")

N = 1000 # number of experiments
total_flips = 0
for i in range(N):
    flips = get_flips(n, k)
    total_flips += flips
    print("flips:", str(flips).rjust(4), \
          " E:", float(total_flips) / (i + 1))
\end{console}
}

I ran the above for $p = 1/10$ and $k = 10$ and got about $100$.
When I ran it at $p = 1/15$ and $k = 20$, I got approximately $300$.
So, experimentally, it seems that the expected number is
\[
\frac{k}{p}
\]

Here's the solution:
If you toss the coin once, the expected number of heads is $p$.
If you toss the coin twice, the expected number of heads is $p + p = 2p$.
In general if $X_i$ is the indicator
random variable for the $i$--th toss being a head,
then we want to find $i$ such that
\[
E[X_1 + X_2 + \cdots + X_i] = k
\]
Since
\[
E[X_1 + X_2 + \cdots + X_i] = E[X_1] + E[X_2] + \cdots + E[X_i] = ip
\]
So to get $k$ heads,
\[
ip = k
\]
and therefore
\[
i = k/p
\]
i.e., on the average, the number of tosses to get $k$ heads
is $k/p$. \qed

    
