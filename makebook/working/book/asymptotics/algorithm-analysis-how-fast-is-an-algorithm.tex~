\section{Algorithmic Analysis: How Fast is an Algorithm?}

Why do we study asymptotics? For the computer scientists,
asymptotics tell us tell us what to focus on and what to ignore.
It creates a useful classification of functions that grow in the
same manner. 

For instance look at the following algorithm that
computes the sum of integers from 1 to $n$ (the value of $n$ is
stored in variable $n$) and the sum is stored in
$s$.
\begin{Verbatim}[frame=single]
s = 0
for i = 1, 2, ..., n:
     s = s + i
\end{Verbatim}
This algorithms solves the following problem:
\[
P(n): \text{\lq\lq Compute the sum of integers from $1$ to $n$''}
\]
This problem is of course made up of many specific problem instances.
For example it includes: 
\[
P(10): \text{\lq\lq Compute the sum of integers from $1$ to $10$''}
\]
and
\[ 
P(1135452): \text{\lq\lq Compute the sum of integers from $1$ to $1135452$''}
\]

Our algorithm:
\begin{Verbatim}[frame=single]
s = 0
for i = 1, 2, ..., n:
     s = s + i
\end{Verbatim}
is an algorithmic solution to our problem $P(n)$.
(It's not the only one.)

In the above, you can think of $n$ as the {\it size} of each problem instance.
And of course you would expect, without even analyzing our
algorithm in detail, that the above algorithm will need more time
for a large $n$. Correct?

Different people design different algorithms to solve the same problem.

We are interested in measuring how fast an algorithm runs
so that we can pick the best.
It's clear that the crucial thing to focus on is the performance
of an algorithm when $n$ is large.
After all, the sum from 1 to 3 is easy --
in fact we can do $1+2+3$ in our heads and not bother with
running a program at all, right?
It takes more time just to let your computer boot up or wake up!!!

In the real world, 
after the algorithm is designed (and you've checked that it's 
correct!!!), 
you still have to implement the algorithm with a programming language
and run the program on a specific computer.
In the real world, the performance of the algorithm can be measured by the
time taken by the computer to run the program.
By \lq\lq time taken'' in this case, I meant measuring time with a
watch or the clock.
This is sometimes called \lq\lq wall-clock'' time.

On some OS, you can also measure processor time for a program, i.e.,
the amount of time that the program actually uses the CPU.

However using the wall-clock or processor time
is problematic because
it depends on the hardware used,
the programming language used,
the operating system, etc.
(No, it does not depend on how many planets are lined up.)

What I want to do is to measure the performance independent of 
external factors, i.e., I want to measure the performance of the 
algorithm.
This does not mean that the external factors are not important.
But rather, we want to solve the performance issue at the root first.
In fact this is usually {\it the} most important factor in the 
performance of any software.

Now let's get back to our 
sum from 1 to $n$ algorithm and see how we can measure the 
runtime performance of an algorithm without even running it on a piece of
hardware.

I'm going to rewrite my algorithm like this (apologies
to those who disapprove of goto statements):
\begin{samepage}
\begin{Verbatim}[frame=single]
         s = 0                     
         i = 1                     
LOOP:    if i > n:                  
             goto ENDLOOP          
         s = s + i                 
         i = i + 1                 
         goto LOOP                 
ENDLOOP:
\end{Verbatim}
\end{samepage}
Now I'm going to attach time taken to execute each statements:
\begin{Verbatim}[frame=single]
         s = 0                     time t1 
         i = 1                     time t2 
LOOP:    if i > n:                 time t3  
             goto ENDLOOP          time t4
         s = s + i                 time t5
         i = i + 1                 time t6
         goto LOOP                 time t7
ENDLOOP:
\end{Verbatim}
This is how you read the above time \lq\lq accounting'':
The statement 
\begin{Verbatim}[frame=single]
         s = 0                     time t1
\end{Verbatim}
takes $t_1$ seconds (or whatever unit of time
you like ... you'll see later that the specific value of 
$t_1$ and the units are not important at all).
For the \verb!if!--statement
(which is made of the header and body):
\begin{Verbatim}[frame=single]
LOOP:    if i > n:                 time t3  
             goto ENDLOOP          time t4
\end{Verbatim}
it takes time $t_3$ for the \verb!if!--statement
to compute the boolean value of $i > n$ and then to decide
to execute \verb!goto ENDLOOP! or not.
So if the boolean condition is true, then the time taken for the 
whole \verb!if! statement is $t_3 + t_4$;
if the boolean condition is false, then the time taken 
is $t_3$.

Note that that times taken to execute each of the above
statement, $t_1$, $t_2$, ... are
{\it constants with respect to} $n$.
What this mumbo-jumbo meant was, 
whether I run the above algorithm
with $n = 10$ or $n = 1000000$, 
the time taken this:
\begin{Verbatim}[frame=single]
         s = 0                     time t1
\end{Verbatim}
is still $t_1$.
Make sense, right?


Next, we count the number of times each statement is executed:
\begin{Verbatim}[frame=single]
         s = 0                     time t1 (once)
         i = 1                     time t2 (once)
LOOP:    if i > n:                 time t3 (n + 1 times) 
             goto ENDLOOP          time t4 (once)
         s = s + i                 time t5 (n times)
         i = i + 1                 time t6 (n times)
         goto LOOP                 time t7 (n times)
ENDLOOP:
\end{Verbatim}
For instance
\begin{Verbatim}[frame=single]
         s = 0                     time t1 (once)
\end{Verbatim}
is executed once (regardless of the value of $n$).
For the \verb!if!--statement
\begin{Verbatim}[frame=single]
LOOP:    if i > n:                 time t3 (n + 1 times) 
             goto ENDLOOP          time t4 (once)
\end{Verbatim}
since $i$ runs through 
$1$, $2$, ..., $n - 1$, $n$, $n + 1$
(the boolean condition evaluates to true for the first $n$
values and false for the last value of $n + 1$),
the header of the \verb!if! is executed $n + 1$ times
and the body is executed only once.

Finally (phew!) we compute the time taken to execute the code.
The total time taken is
\begin{align*}
 \text{Total time}
 &= t_1
    + t_2 
    + (n + 1) t_3 
    + t_4
    + n (t_5 + t_6 + t_7)\\
 &= t_1 
    + t_2 
    + t_3 
    + t_4 +
    n( t_3 + t_5 + t_6 + t_7 )
\end{align*}
Therefore the total time taken is 
\[
An + B
\] 
for some constants
$A$ and $B$. 
Note that
$t_1,t_3,t_3, ...$, and therefore $A$ and $B$,
depends mainly on the machine that is executing the algorithm

It's common to use $T(n)$ to denote the runtime of an algorithm.
So I might write:
\begin{align*}
T(n)
&= t_1 
    + t_3 
    + t_3 
    + t_4 +
    n( t_3 + t_5 + t_6 + t_7 )
\\
&= An + B
\end{align*}

If I'm talking about several algorithmic runtimes together
I would decorate the $T(n)$ notation.
For instance I might write
\[
T^P(n)
\]
or
\[
T^{\operatorname{sum-to}}(n)
\]
or
\[
T^{\operatorname{CONVERT-DIRT-TO-GOLD}}(n)
\]

Now as $n$ grows (and this is the situation we do want 
to worry about), $An$ is of course
going to be larger than $B$. 
So ultimately the time taken to carry
out the algorithm is roughly the function $An$.
And since we don't really care to specify the hardware we're using,
we can also fudge away the constant $A$ and conclude the 
time take to run our algorithm or program is roughly 
(or rather proportional to) the function
\[
n
\]


The technical thing to do is this:
We write 
\[
T(n) = An+B = O(n)
\]
and say \lq\lq $T(n)$ is the big-O of $n$''.
The big-O tells the reader that 
we're only expressing what the runtime function looks like
for large $n$ and when we ignore multiples.
Make sure you see the difference between
\[
T(n) = O(n)
\]
and 
\[
T(n) = n 
\]
(which is wrong).

There are therefore {\it two} elements to measuring the runtime
performance of an algorithm:
\begin{itemize}
\item[(1)] You need to be able to compute the runtime as a formula
in the size of the problem (which in the above is $n$)
\item[(2)] You need to do some fudging to get an \lq\lq approximation'',
i.e., the big-O of the function from (1).
\end{itemize}

Notice in the above example, the fudging {\it simplifies} the function from
\[
An + B
\]
to 
\[
n
\]

Now, as I said before,
I'm ignoring multiples so that 
I'm considering $An$ the same as $n = 1 \cdot n$.
Of course I could have chosen $2n$ as well.
But since I consider all multiples the same, I prefer to use $n$
since it's simpler than $2n$.

Let me summarize the above steps carried out in the computation
of the big-O of the runtime of our sum to $n$ algorithm:

STEP 1. First I assign times to each statement and 
compute the time taken to be
\begin{align*}
T(n)
 &= t_1 
    + t_2
    + (n + 1) t_3 
    + t_4
    + n (t_5 + t_6 + t_7)\\
 &= t_1
    + t_2 
    + t_3 
    + t_4 +
    n( t_3 + t_5 + t_6 + t_7 )
\end{align*}

STEP 2. I clean up and say 
that the time taken is a function of the form
\[
T(n) = An + B
\] 
where $A$ and $B$ are constants.

STEP 3.
The first fudging step is where I looked at the functions $An$ and $B$
and conclude that for large $n$, the function is roughly the function
\[
An
\]

STEP 4.
The second fudging step is when I throw away the $A$ (i.e., replace
the $A$ with $1$) because
the constant $A$ is hardware dependent and say that the 
time taken is roughly the function
\[
n
\]
and I conclude with this statement:
\[
T(n) = O(n)
\]

In general, to compute the big-O of any given function $f(n)$,
\begin{itemize}
\li You only keep the term that grows the fastest because in the long run 
(i.e. for large $n$), the growth of $f(n)$ is determined by this term.
Typically, your runtime function might have more than 2 terms.
\li You replace constant(s) by 1 because different constants
indicates different hardware being used.
\end{itemize}
Later we'll see that there are other simplifications and computational
tools.
The above steps are good enough for now.

OK, let me show you {\it graphically} what the fudging does to a function.
Here are the plots of $y = 10n, y = 10n + 10, y = 10n + 50$:
\begin{python}
from latextool_basic import *
p = FunctionPlot(vars=['n'], domain='0:10', num_points=2)
p.add('10*n', color='red', python=1)
p.add('10*n + 10', color='blue', python=1)
p.add('10*n + 50', color='green', python=1)
print p
\end{python}

(I'm not labeling the graphs
because you should be able to tell which is which ... right?)
For small values of $n$, i.e., $0 \leq n \leq 10$, the functions
are different and separated from each other.
But now if I increase the values for $n$, say, $0 \leq n \leq 100$, 
they look like this:
\begin{python}		
from latextool_basic import *
p = FunctionPlot(vars=['n'], domain='0:100', num_points=2)
p.add('10*n', color='red', python=1)
p.add('10*n + 10', color='blue', python=1)
p.add('10*n + 50', color='green', python=1)
print p
\end{python}

And here's the plot for the domain of $0 \leq n \leq 1000$:
\begin{python}
from latextool_basic import *
p = FunctionPlot(vars=['n'], domain='0:1000', num_points=2)
p.add('10*n', color='red', python=1)
p.add('10*n + 10', color='blue', python=1)
p.add('10*n + 50', color='green', python=1)
print p
\end{python}
All three graphs more or less collapse into a single line, right?
You see that for large values of $n$,
the functions:
\[
y = 10n, \,\,\,\,\,
y = 10n + 10, \,\,\,\,\, 
y = 10n + 50
\]
really behave very much like each other: they all grow as fast as $10n$.

The second fudging step is when I throw away the $A$ 
in the function
\[
An
\]
to get the function
\[
n
\]
because
the constant $A$ is hardware dependent.
Here's a plot of $y = n$, $y = 10n$, $y = 100n$
for $0 \leq n \leq 10$:
\begin{python}
from latextool_basic import *
p = FunctionPlot(vars=['n'], domain='0:10', num_points=2)
p.add('n', color='red', python=1)
p.add('10*n', color='blue', python=1)
p.add('100*n', color='green', python=1)
print p
\end{python}

For $1000 \leq n \leq 10000$:
\begin{python}
from latextool_basic import *
p = FunctionPlot(vars=['n'], domain='1000:10000', num_points=2)
p.add('n', color='red', python=1)
p.add('10*n', color='blue', python=1)
p.add('100*n', color='green', python=1)
print p
\end{python}
They are constant multiples of each other.
And remember this important fact:
I don't care about multiple differences since 
in the case of functions measuring resource use (time in our case)
it just mean that there's a difference in the hardware used.

Also, although they look like they are separated (because of their multiples),
you will see later that when we compare multiples of $n$ with
multiples of $n^2$ and multiples of $n^3$, you will see that the 
multiples of $n$ clump together closely and away from the multiples of
$n^2$ and $n^3$.
You will also see that multiples of $n^2$ clump up together away from the 
multiples of $n^3$.
I'll show you some examples in the next section.

By the way, you can measure any resource taken up by an algorithm.
For instance you can also measure the space complexity of an
algorithm, i.e., the amount of extra memory needed to carry out 
an algorithm.


\newpage
\begin{ex}
The following computes the sum of squares from $1^2$ to $n^2$:
\begin{Verbatim}[frame=single]
s = 0
for i = 1, ..., n:
    term = i * i
    s = s + term
\end{Verbatim}
Here's the program with goto statements and timing for each statement:
\begin{Verbatim}[frame=single]
         i = 1              time t0
         s = 0              time t1
LOOP:    if i > n:          time t2
             goto ENDLOOP   time t3
         term = i * i       time t4
         s = s + term       time t5
         i = i + 1          time t6
         goto LOOP          time t7
ENDLOOP:
\end{Verbatim}
(a) Compute the time taken as a function of $n$, $t_0$, ..., $t_7$.

(b) Simplify the runtime function by giving names $A$, $B$, ...
to the constants of the function from (a).

(c) Fudge away the constants and write down the simplest $g(n)$ such 
that the time in (b) is a big-O of your $g(n)$.
Your $g(n)$ should be either $n$ or $n^2$ or $n^3$ or ...
\qed
\end{ex}

\newpage
\begin{ex}
The following 
sums up all the values in array $x$ of size $n$ and sets
the values of the array to 0:
\begin{Verbatim}[frame=single]
s = 0
for i = 0, ..., n - 1:
    s = s + x[i]
    x[i] = 0
\end{Verbatim}
Assume each of the statements
\begin{Verbatim}[frame=single]
    s = s + x[i]
    x[i] = 0
\end{Verbatim}
take constant time.
Here's the algorithm with goto statements:
\begin{Verbatim}[frame=single]
         s = 0
         i = 0 
LOOP:    if i >= n:         
             goto ENDLOOP          
         s = s + x[i]
         x[i] = 0
         i = i + 1          
         goto LOOP
ENDLOOP:
\end{Verbatim}
(a) Assign constant times to each statement and 
compute the time taken as a function of $n$, $t_0$, ....

(b) Simplify the runtime function by giving names $A$, $B$, ...
to the constants of the function from (a).

(c) Fudge away the constants and write down the simplest $g(n)$ such 
that the time in (b) is a big-O of your $g(n)$.
Your $g(n)$ should be either $n$ or $n^2$ or $n^3$ or ...
\qed
\end{ex}

\newpage
\begin{ex}
The following pseoducode is similar to the above.
\begin{Verbatim}[frame=single]
s = 0
for i = 0, ..., n - 1:
    s = s + x[i]

for i = 0, ..., n - 1:
    x[i] = 0
\end{Verbatim}
(a) Rewrite the above algorithm with
goto statements, assign constant times to each statement and 
compute the time taken as a function of $n$, $t_1$, ....

(b) Simplify the runtime function by giving names $A$, $B$, ...
to the constants of the function from (a).

(c) Fudge away the constants and write down the simplest $g(n)$ such 
that the time in (b) is a big-O of your $g(n)$.
Your $g(n)$ should be either $n$ or $n^2$ or $n^3$ or ...
\qed
\end{ex}

\newpage
\begin{ex}
Let 
\[
f(n) = 100 + 3n^2 + \sin(n)
\]
\begin{itemize}
\item[(a)] Plot the graphs of 
\begin{align*}
y &= 100 \\
y &= 3n^2 \\
y &= \sin(n)
\end{align*}
for $0 \leq n \leq 20$.
\item[(b)] From (a), which term of $f(n)$ grows the fastest
and therefore will ultimately control
the growth of $f(n)$? 
\item[(c)] Compute the big-O of $f(n)$.
\end{itemize}
[NOTE: Using graphs does not really provide a proof.
Big-O requires you to say something about functions
for {\it all} large values of $n$.
Your graph can only show a finite range of $n$.
Later I'll show you how to prove big-O statements.]
\qed
\end{ex}


\newpage
\begin{ex}
Repeat the previous problem with this function:
\[
f(n) = 10000 + \frac{1}{10}n^2 + \sin(n)
\]
Use a sufficiently large domain for $n$.
\qed
\end{ex}


\newpage
\begin{ex}
Repeat the previous problem with this function:
\[
f(n) = 10000 + \frac{1}{10000}n^2 + \frac{n^2}{1 + n}\ln n
\]
Use a sufficiently large domain for $n$.
\qed
\end{ex}
