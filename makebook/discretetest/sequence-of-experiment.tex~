\section{Sequence of Experiments}


Frequently the random experiment you are interested in
can be broken/decomposed into smaller and simpler
experiments.

\subsection{Fixed length}

Now I want to consider the case where the random experiment
is actually made up of a sequence of experiments.
For instance I can consider the random experiment of
rolling two dice.
You can consider this experiment of made up of two experiments:
the first is the experiment of rolling the first die
and the second is the rolling of the second die.

Suppose 
$p_1 : S_1 \rightarrow [0,1]$ 
and 
$p_2 : S_2 \rightarrow [0,1]$ 
are probability distribution functions.
It's natural to define
\[
p: S_1 \times S_2 \rightarrow [0,1]
\]
by 
\[
p(x, y) = p_1(x) \cdot p_2(y)
\]

For the case of rolling two fair dice, clearly
\[
p(x, y) = 1/36
\]
which is of course
\[
p(x, y) = 1/6 \times 1/6 =p_1(x) \cdot p_2(y)
\]

\begin{ex}
Prove that $p = p_1 \cdot p_2$ as defined above is a 
probability distribution function on $S_1 \times S_2$.
\end{ex}

\begin{thm}
If a random experiment involves
carrying out two separate experiments.
Suppose the pdfs are 
$p_1 : S_1 \rightarrow [0,1]$
and
$p_2 : S_2 \rightarrow [0,1]$.
Then
\[
p(\{x\} \times S_2) = p_1(x)
\]
and
\[
p(S_1 \times \{y\}) = p_2(x)
\]
\end{thm}


We say that 
\lq\lq rolling the first die is independent of rolling the second''.
What does this mean?
According to the definition of \lq\lq independence'',
I have to specify two events to begin with.
I don't see two events!

When you hear something like
\lq\lq rolling the first die is independent of rolling the second'' in a
book, this is what was meant:
Define the first event $A$ to be 
the event in $S = S_1 \times S_2$ corresponding to getting
an event in $S_1$.
For instance suppose the event in $S_1$ in $\TWO$.
In other words
\[
A = \{ \TWO \} \times S_2
\]
Likewise suppose the second event is the 
event in $S$ corresponding to the event of $\FOUR$ in $S_2$:
\[
B = S_1 \times \{ \FOUR \}
\]
Then 
\lq\lq rolling the first die is independent of rolling the second''
means that the $A$ and $B$ just defined are independent.
\[
p(A \mid B) = p(A)
\]
Recall that this implies
\[
p(A) p(B) = p(A \cap B)
\]
Note that
\begin{align*}
p(A)
&= p(\{ \TWO \} \times S_2) \\
&= p(\{ (\TWO, y) \mid y \in S_2)) \\
&= \sum_{y \in S_2} p(\TWO, y) \\
&= \sum_{y \in S_2} p_1(\TWO) \cdot p_2(y) \\
&= p_1(\TWO) \sum_{y \in S_2} p_2(y) \\
&= p_1(\TWO)
\end{align*}
and
\[
p(B) = p_2( \FOUR )
\]
We get
\[
p(\{(\TWO, \FOUR)\}) = p(A \cap B) = p_1( \TWO ) \cdot p_2( \FOUR )
\]

\subsection{Sequence of dependent experiments}

There are times when a random experiment involves two experiments
and the second experiment depends on the first.
Here's an example.
Suppose you have two boxes: The first box has 1 red ball and 2 green balls
while the second box has 3 red balls and 5 green balls.
Here's the experiment:
I toss a fair coin.
If I get a head, then I pick a ball from the first box.
If I get a tail, then I pick a ball from the second box.

It's clear that the sample space is
\[
S = \{\HEAD, \TAIL\} \times \{\RED, \GREEN\}
\]
The pdf $p$ of the above random experiment is then
\begin{align*}
  p(\HEAD, \RED) &= 1/2 \cdot 1/3 = 1/6 \\
  p(\HEAD, \GREEN) &= 1/2 \cdot 2/3 = 1/3 \\
  p(\TAIL, \RED) &= 1/2 \cdot 3/8 = 3/16 \\
  p(\HEAD, \GREEN) &= 1/2 \cdot 5/8 = 5/16 
\end{align*}

If the event that I get a head (from the coin toss) independent of the
event that I get a red ball?

Let $A$ be the first event. This means
\[
A = \{(\HEAD, \RED), (\HEAD, \GREEN)\}
\]
Let $B$ be the second event. Then
\[
B = \{(\HEAD, \RED), (\TAIL, \RED)\}
\]
We have
\begin{align*}
  p(A \mid B)
  &= \frac{ p(A \cap B) }{ p(B) } \\
  &= \frac{ p(\{(\HEAD, \RED)\}) }{p(\{(\HEAD,\RED),(\TAIL,\RED)\})} \\
  &= \frac{1/6}{1/6 + 3/16} \\
  &= \frac{8}{17}
\end{align*}
which is not $p(A) = 1/6$.
Therefore $A$ and $B$ are not independent.

\subsection{Variable length}

There are times when you are interested in an
experiment that is made up of a sequence of
experiments where the length $n$ is not fixed.
In fact the questions you are interested in involves
solving for $n$ when a condition is met.

For instance:
\begin{itemize}
  \item How many do I have to roll this die to get a 6?
  \item How many do I have to roll this pair of dice to get the same number?
  \item Etc.
\end{itemize}
