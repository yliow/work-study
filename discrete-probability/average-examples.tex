%-*-latex-*-
\sectionthree{Examples of average computation}
\begin{python0}
from solutions import *; clear()
\end{python0}

Now I'm going to do some very important examples on the
computation of average values.
You'll see the frequent use of indicator random variables.
Recall this exercise from the section on indicator random variable:
Let $X_A$ be an indicator random variable
on $A \subseteq S$.
Then
\[
  \E[X_A] = p(A)
\]
See Exercise \ref{ex-expected-value-of-indicator-rv}
(on page \pageref{ex-expected-value-of-indicator-rv}).

\begin{eg}
  What is the sum of rolling two fair dice?
\end{eg}

Of course you can't answer that exactly.
When you read something like the above question,
you should immediately
replace the question by
\lq\lq What is the \textit{average} of the sum of
rolling two fair dice?"

Do this slowly -- don't rush.
First set up the notation of the problem.
Let $p : S \rightarrow [0, 1]$ be the pdf
of this random experiment.
The random experiment is \lq\lq roll two dice".
The outcomes are
\[
  S = \{(\ONE, \ONE), (\ONE, \TWO), ..., (\SIX,\SIX)\}
\]
and $p$ is uniform, i.e., $p(s) = 1/36$.
The random variable is $X : S \rightarrow \R$
where
\begin{align*}
  X(\ONE, \ONE) &= 2 \\
  X(\ONE, \TWO) &= 3 \\
  \vdots \\
  X(\SIX, \SIX) &= 12
\end{align*}
Therefore the answer is
\begin{align*}
  \E[X]
  &= \sum_{s \in S} X(s) \cdot p(s)
    \\
  &= \sum_{x \in X(S)} x \cdot \Pr[X = x]
    \\
  &= 2 \cdot (1/36) + 3 \cdot (2/36) + \cdots + 12 \cdot (1/36)
\end{align*}
You can check that it's $7$.

\textit{But ... there's another way to do this ...}

Let $X_i$ be the random variable of
number of dots on the top of die $i$ when you roll it.
Let $X = X_1 + X_2$.
Make sure you write down the complete
description of $X_i$ and the associated
probability $\Pr[X_i = \bullet]$.
For instance
\[
  X_1(\ONE, \THREE) = 1
\]
and
\[
  X_2(\ONE, \THREE) = 3
\]
and
\[
  X(\ONE, \THREE) = X_1(\ONE, \THREE) + X_2(\ONE, \THREE) = 1 + 3
\]
Etc.
Do NOT skip the proper definitions of your
sample space, the random variables, etc.
As I mentioned in the previous section,
books like to pretend, while working on $X_1$
that there's only one die and write
\[
  X_1(\ONE) = 1
\]
You should really think of $X_1$ as a function
on the full sample space first.

Then
\[
  \E[X_i] = 1 (1/6) + 2 (1/6) + \cdots + 6 (1/6)
\]
which you can check is 7/2.
Therefore by the linearity of the sum of random variables,
\[
  \E[X] = \E[X_1 + X_2] = \E[X_1] + \E[X_2] = 7
\]
Neat right?
Make sure you read the above two solutions \textit{very}
carefully and understand the benefit of
the linearity of the sum of random variables.
\qed

\input{exercises/sum-of-even-rolls/main.tex}

\input{exercises/n-sided-dice/main.tex}


\newpage

\begin{eg}
  \defproblem{Card Shuffling Problem}.
  Suppose you have a set of $n$ (distinct) cards.
  For simplicity suppose the cards are numbered $0, 1, 2, ..., n - 1$.
  Suppose you then give them a good shuffle so that the desk of cards
  is now random, i.e., each card is  given a random position.
  Here's the question:
  How many cards are in their original position before the shuffle?
  
  This problem is the same as the \defproblem{Hat Check Problem}:
  If $n$ men arrive at a restaurant, leave their hats at the
  reception, and collect their hats on the way out,
  assuming an evil receptionist who gave the hats back randomly
  -- and the $n$ men are half drunk and did not look carefully
  at their hats -- how many men will receive their own hats?

  This is also called the \defproblem{Envelope Problem} where
  an incompetent secretary randomly puts letters into addressed envelopes.
\end{eg}

Of course you can't give me an answer that works for all cases
because the cases are random.
You should interpret the above question as:
\lq\lq \textit{On the average},
how many cards are in their original position before the shuffle?"
What this means is that if you claim that 5 cards retain their
original position, then you are actually saying that
if you perform the above experiment 1,000,000,000 times,
then average across all these 1,000,000,000,000 cases, the number of cards
retaining their original position is 5.

This problem can be
very difficult because there are too many cases to consider.
You would for instance have to consider the case where card 0 is at position 0
and card 0 not at position 0.
Then when consider card 1, you want to worry about the options for card 1
wrt card 0.
For instance if card 0 is at position 0, then card 1 don't have the option
of being shuffle to position 0.
But in the case when card 1 is shuffled to position 1, then card 1
cannot be at position 1.
Etc.
YIKES.

Here's another way to solve this problem.

Formally, the sample space $S$ is the set of
all permutations of $(0, 1, 2, ..., n - 1)$.
Imagine the cards are labeled $0, 1, 2, 3, ..., n - 1$ before being shuffled.
For the case when $n = 4$, say after a shuffle, the outcome is $(2, 0, 1, 3)$.
This represents the deck of cards where:
(card numbered 2, card numbered 0, card numbered 1, card numbered 3).
In other words, the outcomes in $S$ represents all possible shuffled.

The pdf $p : S \rightarrow [0, 1]$ is uniform since shuffle
will result in a permutation of the cards without any preference
for any particular permutation.

Define $X_i$ to be the random variable so that $X_i = 1$
if the $i$--th card is at the $i$--position after the shuffle.
Then $X_i(s_0, s_1, s_2, \ldots, s_{n - 1}) = 1$ if $s_i = i$
if $(s_0, s_1, s_2, \ldots, s_{n - 1}) \in S$.
Note that there are $n$ random variables $X_i$ ($i = 0, 1, ..., n - 1$).
Another thing to note is that $X_i$ is an
indicator random variable on the event
$A_i = \{(s_0, s_1, s_2, ..., s_{n-1}) \in S \mid s_i = i\}$.

Now define another random variable
\[
  X = \sum_{i = 0}^{n - 1} X_i
\]
This is clearly a random variable on $S$.
Also, note that $X(s) = \sum_{i = 0}^{n - 1} X_i(s)$ is the number of $i \in \{0, 1, 2, ..., n - 1\}$
such that $s_i = i$.
See it?
Therefore the required number, the average number of
cards which retained their original position, is 
$\E[X]$.
Therefore
\[
  \E[X]
  =
  \E \left[ \sum_{i = 0} ^{n - 1}  X_i \right ]
  =
  \sum_{i = 0} ^{n - 1} \E \left[  X_i \right]
\]
The key thing is this:
In the above case-by-case (incomplete analysis ... look for the word YIKES) of the cards,
when I consider card 1, I have to worry about where card 1
can go to based on card 0.
However in the computation
of the expected value of a sum,
I can compute the expected value of the terms separately
even when the random variables are not independent -- \textit{this is key}
to understanding the power of the linearity of the expectation of the sum of
random variables.

Now what is $\E[ X_i ]$?
Well that's
\[
  \E[ X_i ] = \sum_{s \in S} X_i(s) p(s)
\]
where $p : S \rightarrow [0,1]$ is the uniform pdf on $S$ i.e., $p(s) = 1/|S| = 1/n!$.
\[
  \E[ X_i ] = \frac{1}{n!}\sum_{s \in S} X_i(s)
\]
Now $\sum_{s \in S} X_i(s)$ is the number of permutations of $S$
where $i$ is fixed.
Therefore
$\sum_{s \in S} X_i(s) = (n - 1)!$.
Hence
\[
  \E[ X_i ]
    = \frac{1}{n!}\sum_{s \in S} X_i(s) \frac{1}{n!} \cdot (n - 1)! = \frac{1}{n}
\]
Hence
\[
  \E[X] = \sum_{i = 0} ^{n - 1} E\left[  X_i \right ] = \sum_{i = 0}^{n - 1} \frac{1}{n} = 1
\]
Which is interesting ... because the average is 1, which does not depend on the
number of cards.

Note that I think of the $X_i$ as a random variable on $S$ (the full sample space).
You can also restrict that to just the \lq\lq sample space of the $i$--th card".
In that case $X_i: \{0, 1, 2, ..., n - 1\} \rightarrow \R$
where $X_i(s_i) = 1$ if $s_i = i$, otherwise it's 0.
The corresponding pdf on $p_i : \{0, 1, 2, ..., n - 1\} \rightarrow [0,1]$ is
$p_i(s_i) = 1/n$.
You will arrive at the same result.

\input{exercises/disc-prob-33/main.tex}
\input{exercises/disc-prob-34/main.tex}
\input{exercises/disc-prob-35/main.tex}
\input{exercises/disc-prob-36/main.tex}
\input{exercises/disc-prob-37/main.tex}

\newpage

\begin{eg}
  \defproblem{Inversion Problem}.
  What is the average number of (pairs of) inversions in a randomly selected
  permutation of $\{0, 1, 2, ..., n - 1\}$?
  For instance, for $(1, 3, 2)$, there is 1 pair of inversion.
  for $(1, 3, 2, 5, 4)$, there are 2 pairs of inversion.
  This one is more complicated:
  there are eight inversions in $(5, 3, 2, 4, 1)$ -- see them? 
  \qed
\end{eg}

Let $S$ be the sample space of
permutations of $0, 1, 2, ..., n - 1$:
\[
  S = \{ (s_0, s_1, \ldots, s_{n - 1})
  \mid
  (s_0, s_1, \ldots, s_{n - 1}) \text{ is a permutation of } (1, 2, 3, ..., n - 1) \}
\]

Let $X_{ij}$ be a random variable
such that
$X_{ij}((s_0, ..., s_{n - 1}))$
is 1 if $s_i, s_j$ is an inversion,
i.e., $s_i > s_j$.
Otherwise $X_{ij}$ is 0.
Therefore the random variable
\[
  X
  = \sum_{0 \leq i < j \leq n - 1} X_{ij}
\]
counts the number of inversion.
Therefore the required number is
\[
\E[X] = 
\sum_{0 \leq i < j \leq n - 1} \E[X_{ij}]
\]
For $0 \leq i < j \leq n - 1$,
\[
  \E[X_{ij}] =
  \frac
  {|\{(s_0,...,s_{n-1}) \in S \mid s_i > s_j \}|}
  {|S|}
\]
Note that we have the following
disjoint union:
\[
  S = 
\{(s_0,...,s_{n-1}) \in S \mid s_i > s_j \}
\dot\cup
\{(s_0,...,s_{n-1}) \in S \mid s_i < s_j \}
\]
(the bijection between the two partitions
is just swapping the 
$i$-- and $j$--coordinates).
Therefore
\[
  \E[X_{ij}] = 1/2
\]
and hence
\[
  \E[X]
  = 
  \binom{n}{2} \cdot \frac{1}{2} 
  = 
  \frac{n(n-1)}{4}  
\]
\qed

\input{exercises/disc-prob-38/main.tex}
\input{exercises/disc-prob-39/main.tex}

\begin{comment}
Let $X_i$ be 1 is $s_i=R,s_{i+1}=G,s_{i+2}=B$.
$\E[X_i] = (1/6)(2/6)(3/6)=1/36$.
Therefore the required
average is $\E[X_0 + X_1 + \cdots + X_{n-3}]
= (n-2)/36$.
For n = 10, answer is 0.2222....

import random; random.seed()

times = 10000000000
n = 10
def ball():
    balls = ['R', 'G', 'G', 'B', 'B', 'B']
    return random.choice(balls)

count = 0
for i in range(times):
    xs = ''.join([ball() for _ in range(n)])
    #print(xs)
    count += xs.count('RGB')
print(float(count)/times)

\end{comment}

\input{exercises/disc-prob-40/main.tex}
\input{exercises/disc-prob-41/main.tex}
\input{exercises/disc-prob-42/main.tex}

\newpage

\begin{eg}
  \defproblem{Coupon Collector Problem}.
  There are $n$ types of coupons for free coffee, one for each of the $n$ different types of coffee at Starbucks.
  One coupon is randomly chosen and placed in identical Kellogg's cereal boxes.
  Since you are a coffee lover, you want to collect all the different $n$ types of coupon.
  (Don't worry. Walmart has a very large number of such Kellogg's cereal boxes with Starbucks coffee coupon
  and they promise you that they do have all the $n$ types of coupons are in their Kellogg's cereal boxes.)
  How many boxes of cereal do you need to buy
  to have a complete collection of all the $n$ types
  of coupons?
\end{eg}

\SOLUTION

First I buy one box.
This means that I have 1 type of coupon.
Let $X_1$ be the number of boxes I need to buy to
get 1 type of coupon.
Clearly $\E[X_1] = 1$.
(In fact $X_1 = 1$ is a constant random variable.)

Next, I buy enough boxes to get the second type of
coupons.
Let $X_2$ be the number of these boxes to get the second type of coupon.
Suppose the sequence of coupons purchase (after the first box) is
\[
  (s_0, s_1, s_2, ...)
\]
The sequence might be
\[
  (0, 0, 0, \text{not 0}, ...)
\]
For this case, $X_2(0, 0, 0, \text{not 0}, ...)) = 4$.
Of course you might be lucky:
$X_2(\text{not 0}, ...)) = 1$.
When I look at $s_0$, the probability that
it is not 0 is $(n - 1)/n$.
Suppose $s_0$ is in fact 0.
Then the probability that
$s_1 \neq 0$ is $(n - 1)/n$.
And the probability that $s_0 = 0$ and
$s_1 \neq 0$ is $(1/n)((n - 1)/n)$.
The probability that $s_2$ is the first coupon
that is not zero is $(1/n)^2((n - 1)/n)$.
The probability that $s_3$ is the first coupon
that is not zero is $(1/n)^3((n - 1)/n)$.
\begin{align*}
  \E[X_2]
  &= 1 \cdot \frac{n-1}{n} + 2 \cdot \left(\frac{1}{n}\right) \frac{n - 1}{n} + 3 \cdot \left(\frac{1}{n}\right)^2\frac{n - 1}{n} + \cdots 
  \\
  &= \left[ 1 + 2 \cdot \left(\frac{1}{n}\right) + 3 \cdot \left(\frac{1}{n}\right)^2 + \cdots \right]
    \left( 1 - \frac{1}{n} \right)
  \\
  &= \left[ 1 + 2 \cdot \left(\frac{1}{n}\right) + 3 \cdot \left(\frac{1}{n}\right)^2 + \cdots \right]
    - \left[\left(\frac{1}{n}\right) + 2 \cdot \left(\frac{1}{n}\right)^2 + 3 \cdot \left(\frac{1}{n}\right)^3 + \cdots \right]
  \\
  &= 1 + \frac{1}{n} + \left(\frac{1}{n}\right)^2 + \cdots
  \\
  &= \frac{1}{1 - 1/n}
  \\
  &= \frac{n}{n - 1}
\end{align*}
Now consider $X_3$, the number of cereal boxes to buy to get the third coupon after you have the first two coupons.
\begin{align*}
  \E[X_3]
  &= 1 \cdot \frac{n-2}{n} + 2 \cdot \left(\frac{2}{n}\right) \frac{n - 2}{n} + 3 \cdot \left(\frac{2}{n}\right)^2\frac{n - 2}{n} + \cdots 
  \\
  &= \frac{1}{1 - 2/n}
    \\
  &= \frac{n}{n - 2}
\end{align*}
In general, consider $X_i$, the number of cereal boxes to buy get the $i$ type of coupon after you have
collected the first $i - 1$ distinct type of coupons.
I get
\begin{align*}
  \E[X_i]
  &= 1 \cdot \frac{n- i + 1}{n}
    + 2 \cdot \left(\frac{i - 1}{n}\right) \frac{n - i + 1}{n}
    + 3 \cdot \left(\frac{i - 1}{n}\right)^2\frac{n - i + 1}{n} + \cdots 
  \\
  &= \frac{1}{1 - (i - 1)/n}
    \\
  &= \frac{n}{n - i + 1}
\end{align*}
Let $X = X_1 + X_2 + X_3 + \cdots + X_n$, i.e., the number of boxes to buy
to get all the $n$ distinct types of coupon.
Then
\[
  \E[X] = \sum_{i=1}^n \frac{n}{n - i + 1} = n \sum_{i=1}^n \frac{1}{n - i + 1} 
\]
Note that the sum is
\begin{align*}
  \sum_{i=1}^n \frac{1}{n - i + 1}
  &= \frac{1}{n} + \frac{1}{n - 1} + \frac{1}{n - 2} + \cdots + \frac{1}{1}
  \\
  &= 1 + \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{n}
  \\
  &= \sum_{i = 1}^{n} \frac{1}{i}
\end{align*}
Hence
\[
  \E[X] = n \sum_{i = 1}^{n} \frac{1}{i}
\]
\qed

Note that for this example, all the $\E[X_i]$'s are different.

\textsc{Aside.}
The sum 
\[
  \sum_{k = 1}^{n} \frac{1}{k}
\]
is called the $n$--the \defone{Harmonic number} and is denoted by $H_n$.
It's known that $H_n$ is very close to $\ln n$ ($\ln$ is log to base $e$).
Specifically
\[
  \lim_{n \rightarrow \infty} (H_n - \ln n) = 0.5772\cdots
\]
is known to be a constant.
This constant
\[
  \gamma = \lim_{n \rightarrow \infty} (H_n - \ln n) 
\]
is called the \defone{Euler-Mascheroni constant}.
$\gamma$ appears in many areas of math, CS, and even physics.
Very little is known about $\gamma$.
It is not even known if $\gamma$ is irrational --
this is a very difficult open problem.
It is known that if $\gamma$ is rational, then the
denominator of $\gamma$ is greater than $10^{242080}$.

\input{exercises/disc-prob-43/main.tex}
\input{exercises/disc-prob-44/main.tex}
\input{exercises/disc-prob-45/main.tex}
\input{exercises/disc-prob-46/main.tex}
  
% in the inversion problem, the index position
% is needed.
% what is the values are set?

\input{exercises/disc-prob-47/main.tex}
\input{exercises/disc-prob-48/main.tex}
\input{exercises/disc-prob-49/main.tex}
\input{exercises/disc-prob-50/main.tex}
\input{exercises/disc-prob-51/main.tex}

\begin{comment}
  $X_{ij}(s)$ is 1 if $s_i = j$ and $s_j = i$.
  Therefore $X = \sum_{i < j} X_{ij} = \sum_{i = 0}{n - 2} \sum_{j = i + 1}^{n - 1} X_{ij}$
  count number of inversions.
\end{comment}
