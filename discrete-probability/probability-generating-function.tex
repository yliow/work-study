\sectionthree{Probability generating function}
\begin{python0}
from solutions import *; clear()
\end{python0}

Let
\[
p: S \rightarrow [0,1]
\]
be a pdf
and $X$ be a random variable on $S$ that maps to $\{0, 1, 2, ...\}$
\[
X: S \rightarrow \{0, 1, 2, ...\}
\]
This gives me a sequence
\[
\Pr[X = n], \,\,\, n = 0, 1, 2, ...
\]
The
\defterm{probability generating function}\index{probability generating function}\tinysidebar{probability generating function\\ pgf}
(\defterm{pgf}\index{pgf})
is the power series
\[
G(x) = \sum_{n \geq 0} \Pr[X = n] x^n
\]
In other words, it's the generating function of the sequence
$\Pr[X = n], \,\,\, n = 0, 1, 2, ...$.
I might write $G_X$ for the probability generating function for $X$.
Note that if the sample space is finite, then the
power series becomes a polynomial.

\input{exercises/pgf-0/main.tex}

Consider the uniform pdf with sample space $S = \{0, 1, 2, ..., n - 1\}$
($n \geq 1$).
The pgf of this uniform pdf is usually written $U_n(x)$:
\[
U_n(x) = \sum_{k = 0}^{n - 1} \frac{1}{n} x^n = \frac{1}{n} \frac{1 - x^n}{1 - x}
\]
The second expression avoid summation.

Note that if I want to compute $U_n'(1)$,
I will have problems because of the $1 - x$ in the
denominator of
$\displaystyle \frac{1}{n} \frac{1 - x^n}{1 - x}$.
Don't worry.
I'll use Taylor series about $1$:
\[
G(x) = G(1) + \frac{G'(1)}{1!} (x-1) + \frac{G''(1)}{2!} (x-1)^2 + \frac{G'''(1)}{3!} (x-1)^3 + \cdots
\]
Let $x = 1 + t$ and I get
\[
G(1 + t) = G(1) + \frac{G'(1)}{1!} t + \frac{G''(1)}{2!} t^2 + \frac{G'''(1)}{3!} t^3 + \cdots
\]
So the coefficient of $t^k$ is
\[
\frac{G^{(k)}(1)}{k!}
\]
For the case of $U_n$,
\begin{align*}
  U_n(1 + t)
  &= \frac{1}{n} \frac{1 - (1 + t)^n}{1 - (1 + t)} \\
  &= \frac{1}{n} \frac{1 - \sum_{k = 0}^n \binom{n}{k}t^k}{t}\\
  &= \frac{1}{n} \frac{\sum_{k = 1}^n \binom{n}{k}t^k}{t}\\
  &= \frac{1}{n} \sum_{k = 1}^n \binom{n}{k}t^{k - 1}\\
  &= \frac{1}{n} \left( \binom{n}{1} + \binom{n}{2} t + \binom{n}{3} t^2 + \cdots + \binom{n}{n} t^{n-1}\right) \tag{*}
\end{align*}
Therefore the $t^k$ coefficient is
\[
\frac{U_n^{(k)}(1)}{k!} = \frac{1}{n} \binom{n}{k + 1}
\]
i.e.,
\begin{align*}
U_n^{(k)}(1)
 &= \frac{1}{n} \binom{n}{k + 1} k! \\
 &= \frac{1}{n} \frac{n!}{(k+1)!(n - k - 1)!} k! \\
 &= \frac{1}{n} \frac{n!}{(k+1)(n - k - 1)!} \\
 &= \frac{1}{n} \frac{(n-k)(n-k+1)\cdots n}{k + 1}
\end{align*}
In particular
\begin{align*}
  U_n(1) &= U_n^{(0)}(1) = \frac{1}{n} \frac{n}{1} = 1 \\
  U'_n(1) &= U_n^{(1)}(1) = \frac{1}{n} \frac{(n - 1)(n)}{2} = \frac{n - 1}{2} \\
  U''_n(1) &= U_n^{(2)}(1) = \frac{1}{n} \frac{(n - 2)(n - 1)(n)}{3} = \frac{(n - 1)(n - 2)}{3}
\end{align*}

For the case of die roll, the standard random variable $X$ maps to $\{1, 2, 3, ..., 6\}$.
So the average die roll is then
\[
\E[X] = \E[U_6 + 1] = \E[U_6] + 1 = \frac{6 - 1}{2} + 1 = 3.5
\]
which we already know.
Note that the coefficients of this pgf is a shift of the
coefficients of $U_6(x)$, i.e.,
\[
G_X = xU_6(x)
\]

Now suppose that $X$ and $Y$ are independent r.v. with values in $\{0, 1, 2, ..\}$ (so that
their pgf $G_X$ and $G_Y$ are defined).
Then
\[
\Pr[X + Y = n] = \sum_{k = 0}^{n} \Pr[X = k, Y = n - k] =  \sum_{k = 0}^{n} \Pr[X = k] \cdot \Pr[Y = n - k]
\]
(The second equality is because $X$ and $Y$ are independent.)
This is the convolution of the sequence $(\Pr[X = k])_{k = 0}^n$ and $(\Pr[X = k])_{k = 0}^n$.
This means that
\[
G_{X + Y} = G_X \cdot G_Y
\]

Earlier I have already shown that the expected value of the sum of rolling two
dice is $7$.
Let's do this using pgf.
If $X = U_n + 1$ is the usual r.v. for a die roll, then
\[
G_{X + X} = G_X \cdot G_X 
\]
Then
\begin{align*}
  E[X + X]
  &= G'_{X+X}(1)\\
  &= G'_X(1) G_X(1) + G_X(1) G'_X(1) \\
  &= 2G_X'(1) G_X(1) \\
  &= 2 \cdot 3.5 \cdot 1 \\
  &= 7
\end{align*}
which we already know from a previous section.

Consider a Bernoulli trial with probabilities $p, q$ (for success and failure respectively)
where $p + q = 1$, i.e., the pdf $p$ is
\begin{align*}
  p(\SUCCESS) = p \\
  p(\FAILURE) = q \\
\end{align*}
Suppose the r.v. is
\begin{align*}
  X: \{ \SUCCESS, \FAILURE \} &\rightarrow \{0, 1\} \\
    X(\SUCCESS) &= 1 \\
    X(\FAILURE) &= 0 \\
\end{align*}
The pgf is
\[
G_X(x) = q + px
\]
Suppose I'm interested in the probability of getting $k$ heads when I
toss $n$ coins, i.e., I'm interested in the probability
distribution of $Y = X + \cdots + X$ ($n$ terms).
The pgf is $G_X \cdots G_X$ ($n$ factors), i.e.,
\[
G_X \cdots G_X = (q + px)^n = \sum_{k = 0}^n \binom{n}{k} p^kq^{n-1} x^k 
\]
Therefore the probability of getting $k$ heads in $n$ tosses is
\[
\Pr[Y = k] = \binom{n}{k} p^kq^{n-1} x^k
\]
which we already know from an earlier section.
This is the Binomial distribution.

The above computations lead to information I have already mentioned.
Now let's look at a  problem that you have not seen before.

What if I'm interested in the average number of die rolls
to get \textit{two consecutive sixes}?
Here's such an experiment:
\[
?, \SIX, ?, ?, ?, \SIX, \SIX
\]
where $?$ is not six.
The relevant r.v. $X$ on this experiment gives me:
\[
X(?, \SIX, ?, ?, ?, \SIX, \SIX) = 7
\]
i.e., number of rolls to get the first two sixes.
Without more tools, you would have to enumerate all possible
scenarios and compute the sum, more or less using
$\E[X] = \sum_{k \in X(S)} k \cdot \Pr[X = k]$:
\[
\E[X] = 2pp + 3qpp + 4(qqpp + pqpp) + 5(qqqpp + qpqpp + pqqpp) + \cdots
\]
Try a few terms yourself say up to the probability for 7 tosses.
See the nightmare?
The problem is first of all writing a complete
$k(...)$ for the case of $k$ tosses and secondly how to organize
the infinite number of terms in such a way that you can sum of the terms.
Let's try using pgf instead ...

Let $G_X$ be the pgf of this $X$.
This $(?, \SIX, ?, ?, ?, \SIX, \SIX)$
will contribute to the coefficient of $x^7$
\[
( \cdots + qpqqqpp + \cdots )x^7
\]
where $p = 1/6$ and $q = 5/6$.
Viewing $qpqqqpp$ and other similar coefficient
in $G_X$ of this problem as strings,
I see quickly that these strings are concatenation of
$q$ and $pq$ followed by $pp$.
As a regular expression these are strings generated by
\[
(q \cup pq)^* pp
\]
(The regular expression $(q \cup pq)^*$ produces strings with no
$pp$ as substring.)     
So
\[
G(x) = \sum_{n = 0}^\infty (qx + pqx^2)^n ppx^2
\]
Hence
\begin{align*}
  G(x)
  &= p^2x^2 \sum_{n = 0}^\infty (qx + pqx^2)^n \\
  &= p^2x^2 \frac{1}{1 - (qx + pqx^2)} \\
  &= \frac{p^2x^2}{1 - qx - pqx^2} 
\end{align*}
Now I compute the expected value of this $X$:
\begin{align*}
  \E[X]
  &= G'(1) \\
  &= p^2 \left( \frac{d}{dx} \ \frac{x^2}{1 - qx - pqx^2} \right) \biggl|_{x = 1} \\
  &= p^2 \cdot \frac{2x(1 - qx - pqx^2) - x^2(-q -2pqx)}{(1 - qx - pqx^2)^2} \biggl|_{x = 1} \\
  &= p^2 \cdot \frac{2(1 - q - pq) - (-q -2pq)}{(1 - q - pq)^2} \\
  &= p^2 \cdot \frac{2 - 2q - 2pq + q + 2pq}{(p - pq)^2} \\
  &= p^2 \cdot \frac{2 - q}{(p(1 - q))^2} \\
  &= p^2 \cdot \frac{1 + p}{p^4} \\
  &= \frac{1 + p}{p^2} \\
  &= \frac{1}{p^2} + \frac{1}{p} 
\end{align*}

In general if I perform a sequence of Bernoulli trails (where success has probability $p$),
then the average number of trails to get two consecutive successes is
\begin{align*}
  \E[X]
  &= \frac{1}{p^2} + \frac{1}{p} \\
\end{align*}
For instance in the case of rolling a fair die, the average number of rolls
to get two sixes if $42$.
The number of coin tosses to get two consecutive heads is $6$.

This problem is much harder than the earlier computation of
expected value.
That's because in earlier examples I compute the expected value
\textit{one} term at a time based on the value of the r.v.:
\[
\E[X] = \sum_{k \in X(S)} \Pr[X = k] \cdot k
\]
In the above problem, each term of $G(x)$
\[
G(x) = \sum_{n = 0}^\infty (qx + pqx^2)^n ppx^2
\]
is of the form
\[
(qx + pqx^2)^n ppx^2
\]
which involves the coefficients of \textit{many} $x$--terms.

\input{exercises/pgf-00/main.tex}
\input{exercises/pgf-01/main.tex}
\input{exercises/pgf-02/main.tex}

