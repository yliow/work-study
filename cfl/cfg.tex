\sectionthree{Introduction}
\begin{python0}
from solutions import *; clear()
\end{python0}

Recall the big picture again ... given a problem 
$\text{\textsc{Problem}}(n)$, instances of it
will form a set 
\[
L_{\text{\textsc{Problem}}} = 
\{ 
\text{\textsc{Problem}}(n) \mid n \in \N
\}
\]
Using this formulation, if there's an automata $M$ that accepts
$L_{\text{\textsc{Problem}}}$, we can solve the given problem.

We saw in the chapter on regular languages that
there are three \lq\lq devices'' that describes the
same class of languages:
The DFAs, NFAs, and regexes (although they are very different) describe
the class of regular languages.
(We also saw the GNFA but that's more like a device used in the proof
that NFAs can be \lq\lq converted'' to regexes.)
We also saw that there are many languages which are not regular.
This means that if a problem gives rise to a regular language, then
DFAs/NFAs/regexes are not powerful enough to solve this problem.

On the practical side, DFAs/NFAs/regexes can be used to 
recognize certain substrings (not all of course).
This is used in for instance compilers to recognize keywords of a 
programming language.
It's also used to validate user inputs such as in a web form.
DFAs and also used to model AI behaviors in, for instance, games.
Historically, regexes were used to analyze neurons in our brain.

(Although I mentioned DFAs/NFAs/regexes, there are in fact many 
other automatas that accept regular languages.)

Now it's time to go on ...

We need more powerful classes of \lq\lq devices''.
These devices will accept regular languages ... and more.

We'll look at context free grammars and pushdown automatas.
They are used in compilers for instance to build \lq\lq parse trees''
after the source program is broken up into lexemes by regexes.
They are also used to analyze human languages and therefore are
important in many area of AI such as natural language processing.

\newpage
\section{Grammar}

Here's an example of a (context-free) grammar (or CFG for short):
\[
G :
\begin{cases}
S &\rightarrow aSb \\
S &\rightarrow \epsilon
\end{cases}
\]
And here's how you use it.
You start with symbol $S$ and choose any arrow and apply the arrow to $S$
to get a string, then you choose a symbol in the string and find an arrow to
apply to that symbol, etc.
You stop when you see only $a$'s and $b$'s.
So here's a string derived from $S$:
\[
S \implies \epsilon
\]
We get $\epsilon$.

Here's another example
\begin{align*}
S 
&\implies aSb & \text{choosing first arrow}\\
&\implies a\epsilon b & \text{choosing second arrow} \\
\end{align*}
We get $ab$.

Here's another example
\[
S 
\implies aSb \\
\implies aaSbb \\
\implies aa\epsilon bb
\]
We get $aabb = a^2 b^2$.

The set of strings generated by $G$ in the above manner, i.e., 
when you have no more variables in the replacement process, is denoted $L(G)$.
Clearly $L(G) = \{a^n b^n \,|\, n \geq 0\}$.
(Right?)
Recall from the chapter on regular languages that this language is not regular.

The arrows are called \textbf{production rules} (or rules).
There are two types of symbols: $S$ is called a \textbf{variable} and symbols 
such as $a$ and $b$ are called \textbf{terminals}.
There can be more than one variable.
You have to start with a symbol,  the \textbf{start variable}. 
In the above example the start variable is $S$.

You can use any symbol as the start symbol, but $S$ is pretty standard.
Note that there must be finitely many product rules.

Note that the production rules simply allows you to replace one
variable is a string of variables/terminals.

You can write the above like this
\[
G: S \rightarrow \epsilon \,|\, aSb
\]
The $|$ is read \lq\lq or''.

\begin{eg}
Let's try something different.
Consider the following context free grammar:
\[
G : S \rightarrow aS \,|\, bS \,|\, \epsilon
\]
Clearly $\epsilon \in L(G)$ since
\[
S \implies \epsilon
\]
using the last rule. Now using the first and last rule we get
\[
S 
\implies aS 
\implies a\epsilon = a
\]
Therefore $a \in L(G)$. 
Check for yourself that using the second and last rule we get $b \in L(G)$.
What about $baab$? Can we derive $baab$?
\begin{align*}
S 
&\implies bS &\text{second rule} \\
&\implies baS &\text{first rule} \\
&\implies baaS &\text{first rule} \\
&\implies baabS &\text{second rule} \\
&\implies baab\epsilon &\text{third rule} \\
\end{align*}
Since $baab \in L(G)$. 
Your turn: Can you derive $ab^2 a^3$ from this grammar?
What is $L(G)$?
I hope it's clear that $L(G) = \Sigma^*$ where $\Sigma = \{a, b\}$.
\end{eg}

\begin{eg}
The grammar:
\[
S \rightarrow S
\]
obviously cannot generate any string. So the language generated by this 
grammar is $\emptyset$.
\end{eg}


\input{exercises/cfg0/main.tex}
\vspace{2in}

\input{exercises/cfg1/main.tex}

\input{exercises/cfg2/main.tex}

\input{exercises/cfg3/main.tex}
\vspace{2in}


\input{exercises/cfg4/main.tex}
\vspace{2in}


\input{exercises/cfg5/main.tex}
\vspace{2in}

\input{exercises/cfg6/main.tex}
\vspace{2in}

\newpage
\begin{eg}
Let's find a CFG $G$ such that $L(G)$ is the set of strings of the form
$a^n b^{2n}$ for $n \geq 0$.
Here are a few strings in $L(G)$:
\[
\epsilon, abb, aabbbb
\]
Of course you can try this grammar:
\[
S \rightarrow \epsilon \,|\, abb \,|\, aabbbb
\]
That's well and dandy ... but if you include other stinrgs like
$aaabbbbbb$, $aaaabbbbbbbb$, ... you'd get infinitely many rules!
But we're only allowed finitely many rules!!!
Now think about the design of DFAs (or NFAs).
The reason why DFAs/NFAs can generate infinitely many strings despite
the fact that there are only finitely many states is that there are loops:
some part of the computation repeats.
OK ... good ... now look at the strings
\[
abb, aabbbb
\]
Do you see one of them included in the other, as in:
\[
abb, a(abb)bb
\]
If you still don't see it, look at this:
\[
abb, a(abb)bb, a(a(abb)bb)bb
\]
and now this:
\[
abb, a(abb)bb, a(a(abb)bb)bb, a(a(a(abb)bb)bb)bb
\]
Do you see a rippling out effect?
You can see that \lq\lq something'' is spitting out an $a$ on the left
and a $bb$ on the right:
\[
a \biggl( a \bigl( a (a\ldots \leftarrow ? \rightarrow \ldots
bb )
bb \bigr)
bb \biggr)
bb
\]
And this is coordinated: You can't have just $bb$ on the right without
an accompanying $a$ on the left. See it yet? ...

[WARNING: SPOILERS ON THE NEXT PAGE ...]

\newpage
Try this:
\[
S \rightarrow a S bb
\]
Of course when you're done with producing the string, you need to stop:
\[
S \rightarrow \epsilon
\]
The CFG is
\[
S \rightarrow a S bb \,|\, \epsilon
\]
Easy, right?
\end{eg}

\newpage
The above example illustrates an extremely important difference between DFAs
and CFGs.
In executing a DFA, you walk from the start state to an accept state and
each step (transition) you take, you basically process a character.
If you think of that as the producing characters for a string, you see that
the process is spitting out characters to the left, one character at a time.
In the case of a CFG, a variable can be replaced by a string including
terminals and variables.
For instance in the case of 
\[
S \rightarrow a S bb
\]
you see that $S$ keeps spitting out characters on the left and on the right:
\[
S \implies aSbb \implies aaSbbbb \implies \cdots
\]
Of course a variable can be replaced with \textit{many} variables too.
So if I have the following rules:
\begin{align*}
S &\rightarrow aUbVc \\
U &\rightarrow aUbb \\
V &\rightarrow bbVcccc
\end{align*}
I can have the following:
\[
S
\implies aUbVcc
\implies a(aUbb)bV
\implies a(aUbb)b(bbVcccc)
\]
In this case, I have \textit{ two} places that can spit out characters left
and right
\[
...[... \leftarrow U \rightarrow ...]...[...\leftarrow V \rightarrow ...]
\]
And of course the variables can be the same, like so:
\[
S \rightarrow aSbSc, \,\,\, S \rightarrow aSbb, \,\,\, S bbScccc
\]

In the above example
\[
S \rightarrow aUbVc, U \rightarrow aUbb, V \rightarrow bbVcccc
\]
you see $U$ being replaced by a string containing only variable $U$
and $V$ begin replaced by a string containing only variable $V$.
We can even have replacement between $U$ and $V$ like this:
\[
S \rightarrow aUbVc, U \rightarrow aUbb\,|\, V, V \rightarrow bbVcccc\,|\, U
\]

Does you head hurt yet?




\newpage
\begin{eg}
Let's try to see if it's possible to produce this language
\[
L = \{a^m b^n \,|\, m \leq n\}
\]
Well if we have a rule like this
\[
S \rightarrow aSb
\]
(i.e. split one $a$ on the left and one $b$ on the right) we'll get
derivations of the form
\[
S \implies \cdots \implies a^mSb^m
\]
i.e. same number of $a$'s and $b$'s. 
Correct?
Together with $S \rightarrow \epsilon$, we would have the language
$\{a^mb^m \,|\, m \geq 0\}$.
That's the idea of our first grammar and
it's no good for us because the number of $a$'s and $b$'s
in the same. For this example we need the number of $b$'s to be
at least the number of $a$'s.
Now what?

Well think about it ... we want to allow the possibility of more $b$'s
than $a$'s.
That means that you want to allow $S$ to produce $b$ (on the right)
but no $a$'s at all.
That means that you want this as well:
\[
S \rightarrow Sb
\]
Note that you do \textit{ not} want to throw away the option of 
\[
S \rightarrow aSb
\]
(Right? Think about it.)
This means that with the following two rules
\[
S \rightarrow aSb \,|\, Sb
\]
you can derive $a^mSb^n$ with $m \leq n$.
And to kill the $S$ at the final step,
you just need to include $S \rightarrow \epsilon$.
Altogether the grammar is 
\[
S \rightarrow aSb \,|\, Sb \,|\, \epsilon
\]
\end{eg}




\input{exercises/cfg7/main.tex}
\vspace{2in}

\input{exercises/cfg8/main.tex}

\input{exercises/cfg9/main.tex}
\vspace{2in}

\input{exercises/cfg10/main.tex}
\vspace{2in}

\newpage
Now we want to look at languages where the $a$'s and $b$'s are not so
\lq\lq orderly'' ...

\begin{eg}
Write down (if possible) a CFG $G$ such that
\[
L(G) = \{w \in \Sigma^* \mid |w|_a = |w|_b |
\}
\]
where $\Sigma = \{a, b\}$ and $|w|_c$ is the number of $c'$  in $w$
where $c \in \Sigma$.
\end{eg}

This language of course includes 
\[
\{a^n b^n \mid n \leq 0\}
\]
But there's more ... it includes
\[
\{b^n a^n \mid n \leq 0\}
\]
or a mumbo-jumbo such as this word
\[
b^{100}abaabbaaabbba^{100}
\]

\newpage
Here's another example (a famous one) where the $a$'s and $b$'s are not
so orderly.

\begin{eg}
Consider the symbols $($ and $)$.
A string over $($ and $)$ is balanced if it corresponds to a correctly
parenthesized algebraic expression with the variables and numbers removed.
For instance 
\[
(w + (xy))(z + w)
\] 
is a valid algebraic expression.
If you remove $w, x, y, z$, you get the string
\[
(())()
\] 
which is a balanced string.
Replacing the ( and ) by $a$ and $b$, we get the string
\[
aabbab
\]
Of course this is not balanced:
\[
abbaab
\]
since this corresponds to parentheses as follows:
\[
()\underline{)}(()
\]
where I've underlined the leftmost parenthesis that does not match up nicely.
It's clear now that to say that $x$ is a balanced string of the symbol
$($ and $)$ is the same as saying that
\begin{tightlist}
\item The number of $($ in $x$ is the same as the number of $)$.
\item If $y$ is a left substring of $x$ (i.e. a prefix substring of $x$),
then the number of $($ in $y$ is $\geq$ the number of $)$'s in $y$. 
\end{tightlist}

Now let me replace $($ with $a$ and $)$ with $b$ ...

We want to find a CFG that generates
\[
L = \{w \,|\,w \text{ is a balanced expression in $a$,$b$} \}
\]

Whoa!!! Where do we begin? For 
\[
\{a^nb^n \mid n \geq 0 \}
\]
we immediately see strings $a^n b^n$ right away ... but \textit{ this} ... we need to write down an algebraic
expression and then ... !?!

Don't panic. Write down your own examples.
For instance we already have this example:
\[
aabbab
\]
Well this grammar is similar to the one for $\{a^nb^n \,|\, n \geq n\}$.
For one thing 
\[
(^n )^n
\]
is balanced, i.e. $a^nb^n$ is in our language $L$ for $n \geq 0$.
In other words the grammar for $\{a^nb^n \suchthat n \geq 0\}$
\[
G_0: S_0 \rightarrow aS_0b \suchthat \epsilon
\]
must be included in the grammar for $L$.
But the problem is that 
\[
aabbab
\]
is in $L$ but not generated by $G_0$.
But you immediately see that the first part \textit{ is} generated by $G_0$:
\[
\underline{aabb} ab
\]
WAIT A MINUTE!!!
The second part is also generated by $G_0$:
\[
aabb \underline{ab}
\]
Holy cow! 
Is this fact crucial or is this just a red herring?
Well if we use the above example as a guide, maybe our grammar is
\begin{align*}
S &\rightarrow S_0 S_0 \\
S_0 &\rightarrow a S_0 b \suchthat \ep
\end{align*}
It does generate out example string (check!)
but it wouldn't work if we have three groups like this:
\[
aabbabab
\]
which corresponds in terms of parentheses to this:
\[
(())()()
\]
which is clearly balanced.
Well we need to be able to produce any number of $S_0$.
Good idea ... so we modify our grammar to get this:
\begin{align*}
S &\rightarrow SS_0 \st \ep \\
S_0 &\rightarrow aS_0 b \st \ep
\end{align*}
The first rule now allows us to derive this
\[
S \implies \cdots \implies S_0 S_0 S_0 S
\]
(in fact any number of $S_0$) and we can kill the last $S$ using
$S \rightarrow \ep$.
After this we just make the first $S_0$ produce $aabb$, the second $S_0$
produce $ab$, and the third $S_0$ produce $ab$.
Yeah!

Now let's try a few more examples (because you're not the overly optimistic
type right?) such as
\[
()()(), ((())), (())(), ...
\]
Go ahead and do it. [PAUSE]

Unfortunately the grammar does not work for this:
\[
aababb
\]
which corresponds in parentheses to the string $(()())$.

If you step back and ask yourself what the grammar
\begin{align*}
S &\rightarrow SS_0 \st \ep \\
S_0 &\rightarrow aS_0 b \st \ep
\end{align*}
can really produce you realize that it does this:
It generates any number of $S_0$'s and each $S_0$ can generate
strings of the form $a^nb^n$.
In other words the grammar can generate string that look like this:
\[
aaabbb \cdot aabb \cdot aaaabbbb \cdot ab
\]
So \textit{ of course} it cannot generate the string
\[
aababb
\]
which corresponds in parentheses to the string $(()())$.

The problem with the above grammar is that once you get to $S_0$, you must
immediately product $a^n$ and $b^n$, left and right.
Now $S$ produce the $S_0$.
What we can do is to allow $S_0$ to produce like this:
\[
S_0 \rightarrow S_0 S_0
\]
then $S_0$ can become \textit{ two} sources of producion of $a^n$ and $b^n$:
\[
S_0 \implies S_0 S_0 \implies \cdots \implies 
a \cdots S_0 \cdots b \,\,\, \cdot \,\,\, a \cdots S_0 \cdots b 
\]
Verifies that it works.

Now if you think about it carefully ... 
how do we write mathematical expressions containing parentheses?
Of course you have 
\[
(a + b)
\]
and you also have
\[
((a + b))
\]
But in general, we use parentheses to indicate which operator to use
during an evaluation:
\[
((a+b)*(c - d))
\]
From the last mathematical expression you see that \lq\lq a parenthesized 
expression'' can contain a parenthesized expression:
\[
( \cdots () \cdots )
\]
\textit{ and} a \lq\lq parenthesized expression'' is made up of the concatenation
of two:
\[
( \cdots ) 
( \cdots ) 
\]
which comes from the mathematical expresion $( \cdots ) + ( \cdots )$
(or with $+$ replaced by $-, \cdot, /$. Right?
And there are no others except for the empty string.
Now if you think carefully about it, you realize that this is enough:
\[
S_0 \rightarrow S_0 S_0 \st aS_0b \st \ep
\]
which the start symbol is $S_0$.
(Recall that $S \rightarrow S S_0$ is only ro produce multiple $S_0$'s;
since we have $S_0 \rightarrow S_0 S_0$, the rule $S \rightarrow S S_0$
becomes redundant.)

Again if $P$ is a balanced expression then either
\begin{tightlist}
\item $P$ is the empty string
\item $P$ is $(P')$ where $P'$ is a balanced expression
\item $P$ is $P'P''$ where $P'$ and $P''$ are both balanced expressions.
\end{tightlist}
Or, if you follow the format of defining regex ... you would say this ...

A balanced expression in $a$ and $b$ is a string produced (only) 
in the following way:
\begin{tightlist}
\item $\ep$
\item If $P$ is a balanced expression, then $aPb$ is also balanced expression
\item If $P$ and $P'$ are balanced expressions, then 
      $PP'$ is also a balanced expression
\end{tightlist}

With this recursive definition of a balanced expression we could have avoided
all the experimentation and errors and arrived at the right grammar 
immediately.
However ... realize this:
insights are not easily earned by just pure abstract thought.
Also pure abstract thought without experimentation to keep the ideas in check
can also be dangerous.

Anyway ... read the above example again \textit{ very} carefully.
\end{eg}




\input{exercises/cfg11/main.tex}

\input{exercises/cfg12/main.tex}

\input{exercises/cfg13/main.tex}

\newpage
\section{Formal Definition of Context Free Grammars}

Now for some high tech math jargon ...

\begin{defn}
A \textbf{context-free grammar} $G$ is $(V, \Sigma, P, S)$ where
\begin{mylist}
 \item[$\bullet$] $V$ is a finite set of \textbf{variables} (or non-terminals)
 \item[$\bullet$] $\Sigma$ is a finite set of \textbf{terminals}
 (some authors use $T$ instead of $\Sigma$ for this set)
 \item[$\bullet$] $P$ is a finite set of
 \textbf{production rules} of the form
  \[
  A \rightarrow \alpha
  \]
  where $A \in V$ and $\alpha$ is a string over $V \cup \Sigma$, i.e.,
  $\alpha \in (V\cup \Sigma)^*$
 \item[$\bullet$] $S \in V$ is the start symbol
\end{mylist}
The strings in $(V \cup \Sigma)^*$ are called
\textbf{sentential forms}.

I will write \textbf{CFG} for context-free grammar.

Let $\alpha_1, \alpha_2 \in (V \cup \Sigma)^*$.
We write $\alpha_1 \implies \alpha_2$ if there is a variable $A$ in $\alpha_1$
and a production rule $P: A \rightarrow \beta$ for $A$ such that 
when the $A$ in $\alpha_1$ is replaced by $\beta$, we get $\alpha_2$.
In other words, we can write $\alpha_1 = \alpha_1' A \alpha_1''$ where
$\alpha_1', \alpha_1'' \in (V \cup \Sigma)^*$ and 
\[
\alpha_1 
= \alpha_1' A \alpha_1'' \implies \alpha_1' \beta \alpha_1'' = \alpha_2
\]
If this is the case, we say that $\alpha_1$ derives $\alpha_2$.
\end{defn}


\begin{defn}
Given a CFG $G$, we define $\implies^{\hspace{-0.3cm}*}$ we follows:
\begin{tightlist}
 \item[$\bullet$] $\alpha \implies^{\hspace{-0.3cm}*}\ \alpha$
 \item[$\bullet$] $\alpha \implies^{\hspace{-0.3cm}*}\ \beta$ if $\beta$ can be derived from 
 $\alpha$ in a finite number of steps.
\end{tightlist}
In other words $\alpha \implies^{\hspace{-0.3cm}*}\ \beta$ if either $\alpha$ \textit{ is} $\beta$
or you can derive $\beta$ from $\alpha$ in a finite number of steps.
So you can think of \lq\lq $\alpha \implies^{\hspace{-0.3cm}*}\ \beta$'' as a shorthand
for \lq\lq $\alpha = \beta$ or $\alpha \implies \cdots \implies \beta$''.
You can define this relation more formally like this.
It is the smallest relation satifying the following two conditions:
\begin{tightlist}
 \item[$\bullet$] $\alpha \implies^{\hspace{-0.3cm}*}\ \alpha$
 \item[$\bullet$] If $\alpha \implies^{\hspace{-0.3cm}*}\ \beta_1$ and 
$\beta_1 \implies \beta$, then 
$\alpha \implies^{\hspace{-0.3cm}*}\ \beta_1$.
\end{tightlist}
\end{defn}
Sometimes it's useful to say that a derivation requires a certain number of
steps.
For instance the following involves 3 derivation steps:
\[
\alpha 
\implies
\beta
\implies
\gamma
\implies
\delta
\]
In this case I will write
\[
\alpha 
\implies^{\hspace{-0.3cm} 3}\  
\delta
\]
Note that there might be many different ways to 
derive $\delta$ from $\alpha$.
I'm just saying that there's \textit{a} derivation of
$\delta$ from $\alpha$ in 3 steps. 

It's not difficult to prove:

\input{exercises/cfg14/main.tex}

\begin{defn}
Let $G$ be a CFG with start symbol $S$.
Then $L(G)$ is the set of strings of terminals that can be derifved from 
$S$, i.e.,
\[
L(G) = \{x \st x \in \Sigma^*, \,\,\, S \implies^{\hspace{-0.3cm}*}\ x \}
\] 
A language is \textbf{context-free} if is is generated by a CFG.
\end{defn}


