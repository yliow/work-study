\section{Linear Recursive Runtime}

So you see that when you measure the runtime of an algorithm
like our Fibonacci function implemented with linear recursion
will give you a runtime function that looks rather similar.
So the next step is to convert the runtime function
to a {\it non-resursive} closed form.

Before I show you how to do that, let's just get a feel the
such runtime functions by calculating with one.
It turns out that it's pretty bad.

Suppose I need to solve the problem of computing this
function (it's somewhat like my Fibonacci).
Note that $g(n)$ is defined by linear recursion.
\begin{align*}
g(n) = 
\begin{cases}
1                     & \text{ if $n = 0$} \\
3                     & \text{ if $n = 1$} \\
2g(n - 1) + 3g(n - 2) & \text{ if $n \geq 2$}
\end{cases}
\end{align*}
Suppose I choose to write my algorithm like this:
\begin{Verbatim}[frame=single]
def g(n):
    if n == 0: 
        return 1
    elif n == 1:
        return 3
    else:
        return 2 * g(n - 1) + 3 * g(n - 2)
\end{Verbatim}
You know that the runtime function would be like this:
\[
T(n) = 
\begin{cases}
A                     & \text{if $n = 0, 1$} \\
2T(n-1) + 3T(n-2) + B & \text{if $n > 1$}
\end{cases}
\]
for some constants $A$ and $B$.
Later I'll show you now to compute an approximation (in the big-O sense)
for $T(n)$.
For now, suppose we compute $g(5)$ using the above 
program, compute like what a program would, executing one statement
or one operation at a time.
This is what you would see:

\begin{align*}
g(5)
&= 2g(4) + 3g(3) \\
&= 2(2g(3) + 3g(2)) + 3g(3) \\
&= 2(2(2g(2) + 3g(1)) + 3g(2)) + 3g(3) \\
&= 2(2(2(2g(1) + 3g(0)) + 3g(1)) + 3g(2)) + 3g(3) \\
&= 2(2(2(2(3) + 3g(0)) + 3g(1)) + 3g(2)) + 3g(3) \\
&= 2(2(2(6 + 3g(0)) + 3g(1)) + 3g(2)) + 3g(3) \\
&= 2(2(2(6 + 3(1)) + 3g(1)) + 3g(2)) + 3g(3) \\
&= 2(2(2(6 + 3) + 3g(1)) + 3g(2)) + 3g(3) \\
&= 2(2(2(9) + 3g(1)) + 3g(2)) + 3g(3) \\
&= 2(2(18 + 3g(1)) + 3g(2)) + 3g(3) \\
&= 2(2(18 + 3(3)) + 3g(2)) + 3g(3) \\
&= 2(2(18 + 9) + 3g(2)) + 3g(3) \\
&= 2(2(27) + 3g(2)) + 3g(3) \\
&= 2(54 + 3g(2)) + 3g(3) & & \text{will this ever end?!?} \\
&= 2(54 + 3(2g(1) + 3g(0))) + 3g(3) \\
&= 2(54 + 3(2(3) + 3g(0))) + 3g(3) \\
&= 2(54 + 3(6 + 3g(0))) + 3g(3) \\
&= 2(54 + 3(6 + 3(1))) + 3g(3) \\
&= 2(54 + 3(6 + 3)) + 3g(3) \\
&= 2(54 + 3(9)) + 3g(3) \\
&= 2(54 + 27) + 3g(3) \\
&= 2(81) + 3g(3) \\
&= 162 + 3g(3) \\
&= 162 + 3(2g(2) + 3g(1)) & & \text{ ... oh no ... here we go again ...}\\
&= 162 + 3(2(2g(1) + 3g(0)) + 3g(1)) \\
&= 162 + 3(2(2(3) + 3g(0)) + 3g(1)) \\
&= 162 + 3(2(6 + 3g(0)) + 3g(1)) \\
&= 162 + 3(2(6 + 3(1)) + 3g(1)) \\
&= 162 + 3(2(6 + 3) + 3g(1)) \\
&= 162 + 3(2(9) + 3g(1)) \\
&= 162 + 3(18 + 3g(1)) \\
&= 162 + 3(18 + 3(3)) \\
&= 162 + 3(18 + 9) \\
&= 162 + 3(27) \\
&= 162 + 81 \\
&= 243 & & \text{PHEW!!!}
\end{align*}

So the execution of $g(5)$ using the above algorithm
seems to indicate that $T(5)$ is huge.
You can see even from this simple simulation that the main
problem is that many function calls were actually repeats.
For instance go ahead and count the number of times
$g(2)$ was executed in the above.

It turns out that except for recursion like this:
\begin{Verbatim}[frame=single]
def h(n):
    if n == 0:
        return 42
    else:
        return 7 * h(n - 1) + n
\end{Verbatim}
where in the recursive part only {\it one} recursive function call was made,
most cases will usually be extremely bad, as in exponential.

It turns out that in general if you have a numeric recursive function
like
\[
T(n) = aT(n - 1) + bT(n - 2) + c
\]
where $a$, $b$, and $c$ are constants,
you can always compute a nice formula for $T(n)$.
A \lq\lq nice'' formula for $T(n)$ that does {\it not} depend on 
$T(i)$ for smaller $i$ but only on $n$ is said to be a \lq\lq closed form'' 
formula.

In fact there are extremely powerful 
tools to compute an {\it exact} closed form for more complicated
recursion such as 
\[
T(n) = n^2 T(n-1) + (1 + n) T(n - 2) + \frac{2}{3}n^2 - 1 
\]
But I'll stick to the simple case of
\[
T(n) = aT(n - 1) + bT(n - 2) + c
\]

Although there are techniques to compute exact closed forms $T(n)$,
remember that we only need the big-O of $T(n)$.

First of all, you ignore the constant $c$ and look at this
recursion instead:
\[
T(n) = aT(n - 1) + bT(n - 2)
\]
You need to know that $T(n)$ must be an exponential
function \lq\lq roughly'' of the form
\[
T(n) = r^n
\]
where $r$ is a constant.
The immediate goal now is to compute $r$.
You substitute this into the above equation ... 
{\it but without the $c$!!!} ...
\[
T(n) = aT(n - 1) + bT(n - 2)
\]
to get
\[
r^n = ar^{n-1} + br^{n-2}
\]
You cross out $r^{n - 2}$ to get
\[
r^2 = ar + b
\]
i.e., a quadratic equation.
This then allows you to solve for $r$.

Let's try this technique on our
\[
T(n) = T(n - 1) + T(n - 2) + d
\]
Let $T(n) = r^n$ and substitute it into
\[
T(n) = T(n - 1) + T(n - 2)
\]
(remember ... ignore the tail-end constant for now!)
to get
\[
r^n = r^{n - 1} + r^{n - 2}
\]
Cross out $r^{n - 2}$ to get
\[
r^2 = r + 1
\]
which gives us the quadratic
\[
r^2 - r - 1 = 0
\]
Using the quadratic polynomial root formula you get
\[
r = \frac{-(-1) \pm \sqrt{(-1)^2 - 4(1)(-1)} }{2}
\]
which gives us
\[
r = \frac{1 \pm \sqrt{5} }{2}
\]
Let 
\[
r_1 = \frac{1 + \sqrt{5} }{2} 
\]
(... this is the one that is about 1.618)
and 
\[
r_2 = \frac{1 - \sqrt{5} }{2}
\]
Then you know that the if the recurrence relation on $T(n)$ is
\[
T(n) = T(n - 1) + T(n - 2)
\]
(we'll worry about the missing $d$ later ...) then
\[
T(n) = \alpha \cdot r_1^n + \beta \cdot r_2^n
\]

Now, $|r_2| = 0.618...$ whereas $r_1 = 1.618...$.
Therefore
\[
T(n) = \alpha \cdot r_1^n + \beta \cdot r_2^n = O(r_1^n)
\]
which means that the algorithm has exponential runtime.

Note that at the stage where you solve for $r$ in 
the quadratic equation in $r$:
\[
r^2 = ar + b \tag{*}
\]
you will have {\it three} cases: $(*)$ has
\begin{itemize}
\item[(a)] two distinct real roots
\item[(b)] two distinct complex (and nonreal) roots
\item[(c)] two roots with the same real value
\end{itemize}
The Fibonacci case of 
\[
r^2 = r + 1
\]
gives us two distinct real roots.

For case (a) and (b) above, the $T(n)$ is just
\[
T(n) = \alpha r_1^n + \beta r_2^n
\]
(Unfortunately for case (b) you would have using complex
numbers.)
For the third case where there is one value $r_1$ for both roots,
the closed form becomes
\[
T(n) = \alpha r_1^n + \beta nr_1^n
\]
so in this case
\[
T(n) = O(nr_1^n)
\]
Of course as for the specific big-O class, you have to 
compute the exact root values.

[EXERCISES]
