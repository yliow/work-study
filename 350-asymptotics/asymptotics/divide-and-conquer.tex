%-*-latex-*-
\sectionthree{Divide-and-conquer algorithms}
\begin{python0}
from solutions import *; clear()
\end{python0}

\newcommand\LEFT{\texttt{left}}
\newcommand\MID{\texttt{mid}}
\newcommand\RIGHT{\texttt{right}}
\newcommand\TARGET{\texttt{target}}
\newcommand\MERGESORT{\texttt{mergesort}}


Another type of recursive runtime function that looks like this:
\[
T(n) = T(n / 2) + A
\]
Such a recursion is called a divide-and-conquer recursion.
These are very different from linear recursion.
An algorithm whose runtime is a divide-and-conquer recursive runtime function
is called ... drumroll ... 
\defterm{divide-and-conquer algorithm} ... (duh).


\newpage
The binary search algorithm (see CISS240 notes) 
is an  algorithm with a runtime of 
\[
T(n) = 
\begin{cases}
A          & \text{if } n = 0 \\
T(n/2) + B & \text{otherwise}
\end{cases}
\]

More generally a divide-and-conquer runtime function looks like this:
\[
T(n) = 
\begin{cases}
A               & \text{for } n = 0, 1, 2, ..., B \\
a T(n/b) + c(n) & \text{otherwise}
\end{cases}
\]
for constants $A, B, a, b$. 
Here $c(n)$ is a function of $n$ (it can be a constant of course).

Mergesort (a sorting algorithm) has a runtime of 
\[
T(n) =
\begin{cases}
A            & \text{if $n = 0, 1$} \\
2T(n/2) + Bn + C & \text{otherwise}
\end{cases}
\]

Before analyzing some divide-and-conquer algorithms,
let me give a high-level overview of such animals.
Divide-and-conquer algorithms are easy to smell from far.

Let's start with the binary search.
(Again see CISS240 notes.)
Suppose you're given a sorted (ascending) array of $n=10$ integers:
\[
\texttt{x} = \texttt{\{1, 3, 4, 6, 7, 8, 10, 12, 13, 17\}}
\]
The goal is to find \verb!target! in $x$, in the sense that
the algorithm should compute the index where the value of 
\verb!target! occurs in $x$.
If that value is not found, then the algorithm returns 
-1.

Suppose $\TARGET = 7$.
The idea behind the binary search is to look at 
$\TARGET$ (or rather the value of $\TARGET$) in the 
middle of $x$.
The middle index is
\[
\MID = n/2 = 10/2 = 5
\]
Since 
\[
\texttt{x[\MID]} = 8 > \TARGET
\]
and since \verb!x! is sorted, $\TARGET$ must be in the left half
of $x$ (if at all), before index $\MID$.
Therefore I'm going to hunt for $\TARGET$ in 
\[
\texttt{x[0..4]} = \texttt{\{1, 3, 4, 6, 7\}}
\]
Note that the size of this new array is $n = 5$, half of the original.
When I repeat the above I get
\[
\MID = n/2 = 5/2 = 2
\]
Since 
\[
\texttt{x[\MID]} = \texttt{x[2]} = \texttt{4} < \TARGET
\]
the $\TARGET$ must be in the right half of $x$, i.e., 
past $\MID$.
The new array is
\[
\texttt{x[3..4]} = \texttt{\{6, 7\}}
\]
which is about half of \verb!x[0..4]!.

Note the the size of the array looks like this:
\[
10 \rightarrow 5 \rightarrow 2
\]
Each time, after looking at $\texttt{x[\MID]}$,
you cut down the array by half: you work on the left half or the right half
of $\texttt{x}$.

Stepping back, you see that the size of work (approximately) halves each time.

The above is the main idea behind the binary search.
However note that it's a waste of time to create new arrays each time we
make a recursive call since the array $x$ is \textit{not modified}.
The correct thing to do is to use the same $x$, i.e.,
pass $x$ by reference and 
also pass two index variable $\LEFT$ and $\RIGHT$
tell the function where's the starting index and ending index
of the current search.
Initially of course $\LEFT = 0$ and $\RIGHT = n - 1$
where $n$ is the size of the array $x$.

See the section on the runtime analysis of the binary search.



\newpage
\begin{ex}
You are given the following recursive function:
\[
f(n) = 
\begin{cases}
2           & \text{if n = 0} \\
3f(n-1) + 5 &\text{otherwise}
\end{cases}
\]
Compute $f(7)$ performing your computation like a computer,
evaluating one operation of performing only one substitution at a time.
\end{ex}




\newpage
\begin{ex}
You are given the following recursive function:
\[
f(n) = 
\begin{cases}
2           & \text{if n = 0} \\
3f(n/2) + 5 &\text{otherwise} 
\end{cases}
\]
Note that \lq\lq $n/2$ '' really 
meant the floor of $n/2$ so technically I should 
really write
\[
f(n) = 
\begin{cases}
  2                 & \text{if n = 0} \\
3f(\floor{n/2}) + 5 & \text{otherwise}
\end{cases}
\]
Compute $f(7)$ performing your computation like a computer,
evaluating one operation of performing only one substitution at a time.
(Recall that floor $\floor{x}$ means go the integer before $x$
and ceiling $\ceiling{x}$ means the integer after $x$.
For instance $\floor{3.4} = 3$ and $\ceiling{3.4} = 4$.)
\end{ex}



Now let's look at a high level overview of mergesort.
It's easier to do an example by hand.
The main idea when you call mergesort with an array $x$,
the function splits $x$ into two arrays of about equal size,
say we call them $y$ and $z$.
Next, we call mergesort on $y$ and save the result in say $a$;
this $a$ will be sorted.
Third, we call mergesort on $z$ and save the result in say $b$;
this $b$ is sorted.
Fourth, we merge the sorted $a$ and $b$ into $c$ and return $c$.
I'll talk about the merge operation in a bit.
In mergesort, if the list $x$ has size 0 or 1, you just return $x$;
this is the base case.

Now for the merge operation.
If you have two sorted array $a$ and $b$ and you want to 
merge them into $c$,
you just scan $a$ and $b$ left-to-right,
picking the smallest to be placed into another array.
Briefly, you put one finger at index $0$ of $a$ and 
another at index $0$ of $b$.
You then look at what you're pointing to,
pick the smaller and dump it to array $c$,
moving your finger past the smaller value.
That's it.

For instance if $a = [2,4,5]$ and $b=[1,2,3]$,
you look at $a[0]$ and $b[0]$.
Since $b[0]$ is smaller, you put $b[0]$ into $c[0]$.
So $c[0] = 1$. $b[0]$ is done and you look at $b[1]$.
We now compare $a[0]$ and $b[1]$. They are the same.
I'll just choose $a[0]$ and put that into $c[1]$.
So now $c[0]=1, c[1]=2$. We now move now to the next element in $a$,
i.e., $a[1]$. We compare $a[1]$ and $b[1]$.
Since $b[1]$ is smaller, I put that into $c[2]$.
I now compare $a[1]$ and $b[2]$.
Since $b[2]$ is smaller, I put that into $c[3]$.
Once an array is finished (in this case $b$), I just put the rest
of what remains in $a$ into $c$.


\newpage
\begin{ex}
  Write a merge function.
  For instance \verb!merge(u, v, w)! where $u,v,w$ are \verb!std::vector< int >!
  objects, and \verb!u,v! are sorted, and you want to merge \verb!u,v! into \verb!w!. 
\qed
\end{ex}



\newpage
Suppose you're given this array:
\[
x = [3, 6, 4, 2, 7, 8, 12, 15, 1, 5]
\]
Let me call mergesort:

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP1. mergesort(x = [3, 6, 4, 2, 7, 8, 12, 15, 1, 5])
First I divide x into two halves y and z and call mergesort(y).
y = [3, 6, 4, 2, 7]
z = [8, 12, 15, 1, 5]
a = mergesort(y) ... WAITING FOR RETURN VALUE FROM STEP2
b = ?
c = ?
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP2. mergesort(x = [3, 6, 4, 2, 7])
I repeat the process on $x = [3, 6, 4, 2, 7]$.
y = [3, 6]
z = [4, 2, 7]
a = mergesort(y) ... WAITING FOR RETURN VALUE FROM STEP3
b = ?
c = ?
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP3. mergesort(x = [3, 6]).}
I repeat the process on $[3, 6]$.
y = [3]
z = [6]
a = mergesort(y) ... WAITING FOR RETURN VALUE FROM STEP4
b = ?
c = ?
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP4. mergesort(x = [3])
Since the size of x is 1, we are in the base case. 
So I return [3] back to STEP3.
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP3. mergesort(x=[3,6]) ... returning from STEP 4 ...
The result of mergesort(y) is stored in a and we call mergesort(z) 
y = [3]
z = [6]
a = [3]
b = mergesort(z) ... WAITING FOR RETURN FROM STEP5 ...
c = ?
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP5. mergesort(x = [6])
Since the size of x is 1, we are in the base case. 
So I return [6] back to STEP3.
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP3. mergesort(x=[3,6]) ... returning from STEP 5 ...
The result of mergesort(z) is stored in b and we merge a, b and store in c 
y = [3]
z = [6]
a = [3]
b = [6]
c = [3, 6]
Return c to STEP2
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP2. mergesort(x = [3, 6, 4, 2, 7])
The return value from STEP3 is stored in a. Call mergesort(z)
y = [3, 6]
z = [4, 2, 7]
a = [3, 6]
b = mergesort(z) ... WAITING FOR RESULT FROM STEP6 ...
c = ?
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP6. mergesort(x = [4, 2, 7])
y = [4]
z = [2, 7]
a = mergesort(y) ... WAITING FOR RESULT FROM STEP7
b = ?
c = ?
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP7. mergesort([4])
Return [4] back to STEP6.
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP6. mergesort(x = [4, 2, 7])
y = [4]
z = [2, 7]
a = [4]
b = mergesort(z) ... WAITING FOR RESULT FROM STEP8 ...
c = ? 
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP8. mergesort(x = [2, 7])
y = [2]
z = [7]
a = mergesort(y) ... WAITING FOR RESULT FROM STEP9
b = ?
c = ?
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP9. mergesort(x = [2])
Return [2] back to STEP8.
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP8. mergesort(x = [2, 7])
y = [2]
z = [7]
a = [2]
b = mergesort(z) ... WAITING FOR RESULT FROM STEP10 ...
c = ?
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP10. mergesort(x = [7])
Return [7] back to STEP8.
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP8. mergesort(x = [2, 7])
y = [2]
z = [7]
a = [2]
b = [7]
c = [2, 7]
Return c to STEP6 
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP6. mergesort(x = [4, 2, 7])
y = [4]
z = [2, 7]
a = [4]
b = [2, 7]
c = [2, 4, 7]
Return c to STEP2
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP2. mergesort(x = [3, 6, 4, 2, 7])
The return value from STEP3 is stored in a. Call mergesort(z)
y = [3, 6]
z = [4, 2, 7]
a = [3, 6]
b = [2, 4, 7]
c = [2, 3, 4, 6, 7]
Return c to STEP1
\end{Verbatim}

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
STEP1. mergesort(x = [3, 6, 4, 2, 7, 8, 12, 15, 1, 5])
First I divide x into two halves y and z and call mergesort(y).
y = [3, 6, 4, 2, 7]
z = [8, 12, 15, 1, 5]
a = [2, 3, 4, 6, 7]
b = mergesort(z) ... WAITING FOR RESULT FROM STEP11 ...
c = ?
\end{Verbatim}

\begin{ex}
Finish the above execution by hand.
\qed
\end{ex}



\newpage
Informally, you see that if $T(n)$ is the time to execute
mergesort with an array of size $n$, the function call
will split the array into two pieces (this requires linear time)
then call mergesort with these two halves (this requires $2T(n/2)$)
and then merge the two return results (this requires linear time). 
This results in a runtime of the form
\[
T(n) = 2T(n/2) + An + B
\]
for some constants $A,B$.

Again, just like binary search, there are ways to prevent
the over-creation of new arrays. 
Therefore memory usage can be improved.
But the above is the main idea behind mergesort.

See later section on mergesort.

As you can see, both binary search and mergesort involves
splitting an array into two halves.
\begin{itemize}
\li In the case of binary search, the algorithm determines which
half to continue with the search (the other is then thrown away), 
resulting in a runtime that looks like
$T(n) = T(n/2) + A$.
\li In the case of mergesort, the mergesort then recursively
work on each half (both halves are used), resulting in 
a runtime that looks like
$T(n) = 2T(n/2) + An + B$.
\end{itemize}

For binary search, of course if you have an array of size $n$, then
in the next iteration of binary search, excluding the value at the \verb!mid! index,
there are \verb!n - 1! values to analyze
and the left and right subarray would have sizes of
either
$\floor{(n - 1)/2}$
or
$\ceiling{(n - 1)/2}$.
But asymptotically when \verb!n! is large,
it's enough to replace these by $n/2$.
So instead of saying 
\[
T(n) = T(\floor{(n - 1)/2}) + A
\]
or
\[
T(n) = T(\ceiling{(n - 1)/2}) + A
\]
we just write the recurrence relation of $T(n)$ as
\[
T(n) = T(n/2) + A
\]
This is the same for the recurrence relation for mergesort.

In general divide-and-conquer algorithms involves splitting
up the work into roughly equal smaller pieces, recursively
performing the algorithm on the smaller pieces with 
possibly work done on the return value from the function call 
to work on the smaller pieces.




\newpage
\begin{ex}
If instead of splitting up an array to 2 pieces in mergesort,
what if I split up the array into 3, call mergesort on the 3 pieces
and then merge the three sorted pieces.
What do you think the runtime is going to look like?
\qed
\end{ex}




\newpage
\begin{ex}
Here's our simple sum to $n$ algorithm:
\begin{Verbatim}[frame=single, fontsize=\footnotesize]
def sum(n):
    s = 0
    for i = 1, 2, 3, ..., n:
        s = s + i
    return s
\end{Verbatim}
Let's do it in a different way.
Here's another function that sums from one point to another:
\begin{Verbatim}[frame=single, fontsize=\footnotesize]
def sum2(m, n):
    s = 0
    for i = m, m + 1, m + 2, ..., n:
        s = s + i
    return s
\end{Verbatim}
Design and write a sum to $n$ function that works recursively like this:
Instead of summing 1 to \verb!n! in a loop, the function
calls \verb!sum2! to sum from \verb!1! to \verb!n/2!, 
calls \verb!sum2! to sum from 
\verb!n/2 + 1! to \verb!n!, 
and finally returns the sum of the two return values.
The base case is when \verb!n! is \verb!1!.
Test your code.
What do you think is the runtime?
\qed 
\end{ex}



\newpage
\begin{ex}
Using the same idea as above, write a \verb!sum! function
that accepts an array (of integers, say) and recursively
splits the array into two, calls the sum function, and return the 
sum of the return values.
The base case is when the array is empty in which case you return 0
or when the array has one value in which case you return that value.
Test your code.
What do you think is the runtime?
\qed
\end{ex}


\newpage
\begin{ex}
Write a recursive \verb!max! that takes an array, splits the
array into two pieces, calls max on the pieces, and returns
the max of the two return value.
I'll let you figure out the base case scenario and what to do. 
\qed
\end{ex}


\newpage
\begin{ex}
Write a \verb!count! function that accepts an array \verb!x!
and a value \verb!v! and returns the numbers of times the value of
\verb!v! occurs in \verb!x!. 
Use the divide-and-conquer strategy.
\qed
\end{ex}

\begin{comment}
Here's binary search for $\TARGET$ in a sorted array \verb!x!:
\begin{Verbatim}[frame=single, fontsize=\footnotesize]
left = 0
right = n - 1
while left <= right:
    mid = (left + right) / 2
    if x[mid] < target:
        left = mid + 1
    elif x[mid] > target:
        right = mid - 1
    else:
        return mid
return -1
\end{Verbatim}

Before I do that, let me rewrite the above in this form:
\begin{Verbatim}[frame=single, fontsize=\footnotesize]
left = 0
right = n - 1

while 1:
    if left > right:
        return -1
    else:    
        mid = (left + right) / 2
        if x[mid] < target:
            left = mid + 1
        elif x[mid] > target:
            right = mid - 1
        else:
            return mid
\end{Verbatim}

You can recast this as a recursion:
\begin{Verbatim}[frame=single, fontsize=\footnotesize]
def binarysearch(x, target, left=0, right=len(x) - 1):
    if left > right:
        return -1
    else:
        mid = (left + right) / 2
        if x[mid] < target:
            binarysearch(x, target, mid + 1, right)
        elif x[mid] > target:
            binarysearch(x, target, left, mid - 1)
        else:
            return mid
\end{Verbatim}

Let $T(n)$ be the runtime where $n$ is the index range that
of the search. For instance in the call of
\verb!binarysearch(x, target, 4, 10)!
the value of $n$ is $10 - 4 + 1$.

Let's compute the runtime.

\begin{Verbatim}[frame=single, fontsize=\footnotesize]
    if left > right:                                t1
        return -1                                   t2
    
    mid = (left + right) / 2                        t3
    if x[mid] < target:                             t4
        binarysearch(x, target, mid + 1, right)     t5
    elif x[mid] > target:                           t6
        binarysearch(x, target, left, mid - 1)      t7
    else:                                           t8
        return mid                                  t9
\end{Verbatim}


First of all, our array \verb!x! is passed by reference.
Each function call to \verb!binarysearch! works on 
\[
\verb!x[left]!, ...,
\verb!x[right]!
\]
Therefore the size \verb!n! of the algorithm is 
\[
n = \RIGHT - \LEFT + 1
\]
The base case occurs when $\LEFT > \RIGHT$
which is of course the same as
\[
n = 0
\]
which is the execution of this part of the function:
\begin{Verbatim}[frame=single, fontsize=\footnotesize]
...
    if left > right:
        return -1

    ...       
\end{Verbatim}
Of course the runtime is constant.
\[
T(0) = A
\]

Otherwise we have the case
$\LEFT \geq \RIGHT$
which is the same as
\[
n \geq 1
\]
Note that
\[
\MID = \floor{\frac{\LEFT + \RIGHT}{2}} 
\]

This is the recursive part. 
There are three cases. The first case results in 
\begin{align*}
T(n) 
&= t1 + t3 + t4 + t5
\end{align*}
Everything is constant except for t5 which is
\[
T(m)
\]
where 
\begin{align*}
m
&= \RIGHT - (\MID + 1) + 1 \\
&= \RIGHT - \MID \\
&= \RIGHT - \floor{\frac{\LEFT + \RIGHT}{2}} \\
&\approx \RIGHT - \frac{\LEFT + \RIGHT}{2} \\
&= \frac{\RIGHT - \LEFT}{2} \\
&\approx \frac{n}{2} \\
\end{align*}

\end{comment}
