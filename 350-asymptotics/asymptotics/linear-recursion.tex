%-*-latex-*-
\sectionthree{Linear Recursion}
\begin{python0}
from solutions import *; clear()
\end{python0}

Recall that a recursive function is a function that calls itself.
For instance here's Fibonacci again:
{\small
\begin{Verbatim}[frame=single,fontsize=\footnotesize]
def fib(n):
    if n == 0 or n == 1:
        return 1
    else:
        return fib(n - 1) + fib(n - 2)
\end{Verbatim}
}
Or in C/C++:
{\small
\begin{Verbatim}[frame=single,fontsize=\footnotesize]
int fib(int n)
{
    if (n == 0 || n == 1):
        return 1;
    else
        return fib(n - 1) + fib(n - 2);
}
\end{Verbatim}
}
Of course this is your standard first Fibonacci implementation. 
(Review your recursion notes from CISS245.)

The form of this function is derived from math.
In your math classes, one would write this function as
\[
f(n) = 
\begin{cases}
1                   & \text{if $n = 0, 1$} \\
f(n - 1) + f(n - 2) & \text{otherwise}
\end{cases}
\] 
In math, one would also say that this definition is piecewise ... 
because there are two pieces in the definition, right?

Each of the following cases
\[
f(0) = 1, \,\,\,\,\, f(1) = 1
\]
is usually called a \defone{base case}.
The case that has recursion
\[
f(n) = f(n - 1) + f(n - 2) , \,\,\,\,\, n > 1
\]
is called the \defone{recursive case} (duh).

How do you analyze the runtime of the above recursive function?
There are many types of recursion.
The above recursion
\[
f(n) = f(n - 1) + f(n - 2)
\]
is an example of a \defone{linear recursion} with fixed degree.
In the Fibonacci case, the \defone{degree} is 2 because
the parameters in the above are
\[
n, \,\,\,\,\, n - 1, \,\,\,\,\, n - 2
\]
and the largest difference between $n$ and the other parameters is $2$.
In general a degree 2 linear recurrrence $g(n)$ looks like this:
\[
g(n) = a \cdot g(n-1) + b \cdot g(n-2)
\]
where $a,b$ are constants.
A degree 3 linear recurrrence $h(n)$ looks like this:
\[
h(n) = a \cdot h(n-1) + b \cdot h(n-2) + c \cdot h(n-3)
\]
where $a,b,c$ are constants.



Here's what you do.
First you let 
\[
T(n)
\]
denote the total time to execute \verb!f(n)!
\textit{including} the time due to the recursive calls.
That includes the function call, the execution of the body (which can
include the time to execute the base case code or
the time due to the execution of the recursive calls)
and the return.

\begin{Verbatim}[frame=single,fontsize=\footnotesize]
def f(n):                                 t1
    if n == 0 or n == 1:                  t2
        return 1                          t3
    else:                                 t4 
        return f(n - 1) + f(n - 2)        t5
\end{Verbatim}

If the value of \verb!n! is \verb!0!, the above function returns 1.
Therefore
\begin{align*}
T(0) &= t_1 + t_2 + t_3 \\
T(1) &= t_1 + t_2 + t_3
\end{align*}
Since we're going to compute big-O, 
we're just going to write
\[
T(0) = T(1) = A
\]
for some constant $A$.
Now for the recursive case ...

What if $n > 1$?
Notice that your function call of \verb!f(n)! will have to make two calls:
a call to \verb!f(n - 1)! and then another call to \verb!f(n - 2)!.
Of course the time taken to execute \verb!f(n - 1)! is $T(n - 1)$.
And the time to execute 
\verb!f(n - 2)! is $T(n - 2)$.
When you get the results of \verb!f(n - 1)! and \verb!f(n - 2)!,
you have to add them together (which takes constant time)
and return the result (which also takes constant time).
Note the ... \textit{VERY IMPORTANT POINT} ... here:

Time $t_5$ depends on \verb!n! and therefore is \textit{not}
constant (with respect to \verb!n!).

Let me do this carefully (and slowly) and include everything.
You'll see later that most of the extra things/details to consider
are actually $O(1)$, i.e. constant time, and therefore 
not that crucial ... but you have to see it to see what's
happening. 
Here's the timing for $t_5$:
\begin{align*}
t_5 
&= \text{[time to compute $n - 1$]} + \text{[time to execute $f(n - 1)$]} \\
&\,\,\,\,\,  + \text{[time to compute $n - 2$]} + \text{[time to execute $f(n - 2)$]} \\
&\,\,\,\,\,  + \text{[time to add the return values of above]} \\
&\,\,\,\,\,  + \text{[time to return the sum]} \\
&= \text{[some constant]} + T(n-1) \\
&\,\,\,\,\,  + \text{[some constant]} + T(n-2) \\
&\,\,\,\,\,  + \text{[some constant]} \\
&\,\,\,\,\,  + \text{[some constant]} \\
&= T(n-1) + T(n-2) + B
\end{align*}
where $B$ is a constant.
(There are other details.
For instance time might be taken to store
the return value of the $f(n - 1)$ function call temporarily
before making the next function call of $f(n-2)$.
The time to store an integer value is constant and does not
change with $n$. 
So again this takes constant time.)

So for the case of $n > 1$,
\begin{align*}
T(n) 
&= t_1 + t_2 + t_4 + t_5 \\
&= t_1 + t_2 + t_4 + T(n-1) + T(n-2) + B \\
&= T(n-1) + T(n-2) + C
\end{align*}
for some constant $C$.

So, voil\`a, all together we have
\[
T(n) 
=
\begin{cases}
A                       & \text{ if $n = 0, 1$} \\
T(n - 1) + T(n - 2) + C & \text{ otherwise}
\end{cases}
\]

Hey! 
That does not give a formula for $T(n)$ in terms of $n$ ...
but in terms of $T(n - 1)$ and $T(n - 2)$!!!

The above \textit{recursive algorithm} gives a
\textit{recursive runtime formula}.

Or we say $T(n)$ satisfies a \defone{recurrence relation}.
Of course in analyzing runtimes, you prefer to put this
$T(n)$ into one of the standard runtime big-O classes such as
\[
O(1), \,\,\,\,\,
O(n^a), \,\,\,\,\,
O(n^b\ln^c n), \,\,\,\,\, 
O(c^n), ...
\]
so that you can tell how fast or slow it runs.
So I need to rewrite a recursive runtime formula into one that is not
recursive (yes, it's possible).
Such a formula is said to be a \defone{closed form formula}. 

\begin{comment}
Not to panic.
All you need to do is to understand
how to do numeric recursion (i.e. substitution).
For instance
\begin{align*}
T(5) 
&= T(4) + T(3) + d \\
&= (T(3) + T(2) + d) + (T(2) + T(1) + d) + d           & & \text{substitute} \\
&= T(3) + 2T(2) + T(1) + 3d                            & & \text{cleanup ...  ur mom told u right?} \\
&= (T(2) + T(1) + d) + 2(T(1) + T(0) + d) + T(1) + 3d  & & \text{substitute} \\
&= T(2) + 4T(1) + 2T(0) + 6d                           & & \text{cleanup} \\
&= T(1) + T(0) + d + 4T(1) + 2T(0) + 6d                & & \text{substitute} \\
&= 5T(1) + 3T(0) + 7d                                  & & \text{cleanup}
\end{align*}

This is all well and good ... but we need $T(n)$, not $T(5)$!!!
Ultimately do you see that $T(n)$ must be of the form:
\[
T(n) = \text{BLAH1} \cdot T(1) + \text{BLAH2} \cdot T(0) + \text{BLAH3} \cdot d
\]
where BLAH1, BLAH2 and BLAH3 must be formulas involving $n$.
Ultimately we're going to ditch the constants $T(1)$, $T(0)$ and $d$.
What matters is really BLAH1, BLAH2, BLAH3.
It turns out that BLAH1 and BLAH2 are of the form
\[
\text{(some constant)}^n
\]
One of the constants is approximately 1.618 while the other is
approximately -0.618.

That already tells you that the runtime of the above 
algorithm to compute the $n$--th Fibonacci number is pretty bad:
it's ...
\[
\text{exponential!!!}
\]

That's why if you use the above to compute $f(i)$ for $i = 0, 1, 2, ...$,
you will see that your program will slow down to almost a grinding halt
after $i=50$.

It turns out that for such problems you can rewrite the program to run in 
linear time, i.e., $O(n)$.
In fact that there are \textit{several} different ways to do that.
I'll talk about this is another set of notes 
(on techniques for re-designing algorithms to
improving runtime) since for this set of notes
I'm only going to focus on measuring runtimes.

Of course I still have not explained how I got this mysterious 1.618.
But I'm going to take a pause here and give you some exercises first ...

First of all, it's clear that a \textit{recursive algorithm} like the above
will give you a corresponding \textit{recursing runtime}.
Here are some exercises to verify that. 


\newpage
\begin{ex}
Using the same technique above, compute a recursive runtime
for the following algorithm.
(Don't worry about writing the formula in closed form.)
\begin{Verbatim}[frame=single,fontsize=\footnotesize]
def f(n):
    if n == 0: 
        return 1
    elif n == 1:
        return 3
    else:
         return 2 * f(n - 1) + 3 * f(n - 2)
\end{Verbatim}
\end{ex}

\newpage
\begin{ex}
Using the same technique above, compute a recursive runtime
for the following algorithm.
(Don't worry about writing the formula in closed form.)
\begin{Verbatim}[frame=single,fontsize=\footnotesize]
def f(n):
    if n == 0: 
        return 5
    else:
        return 7 * f(n - 1) + 2 
\end{Verbatim}
\end{ex}


\newpage
\begin{ex}
Using the same technique above, compute a recursive runtime
for the following algorithm.
(Don't worry about writing the formula in closed form.)
\begin{Verbatim}[frame=single,fontsize=\footnotesize]
def f(n):
    if n == 0: 
        return 7
    elif n == 1:
        return 10
    else:
        return n * f(n - 1) +  f(n - 2)
\end{Verbatim}
\end{ex}



\newpage
\begin{ex}
Using the same technique above, compute a recursive runtime
for the following algorithm.
(Don't worry about writing the formula in closed form.)
\begin{Verbatim}[frame=single,fontsize=\footnotesize]
def f(n):
    if n == 0: 
        return 7
    elif n == 1:
        return 10
    else:
        return f(n - 1) * f(n - 2)
\end{Verbatim}
\end{ex}

%[MORE EXERCISES]

\end{comment}

%\newpage
%\subsection*{Solutions}
%\input{solutions.tex}
