\section{Divide and conquer; master theorem}

There are other types of recurrence relations.
Here's one:
\[
a_n = a_{n/2} + 1
\]
and here's another
\[
a_n = na_{n/3} + \log n
\]

For such recurrences I might write $a(n)$ for $a_n$.
So here's the first example:
\[
a(n) = a(n/3) + 1
\]
Note $n$ is an integer and $a_i$ is usually defined only for 
integer value $i$. 
Therefore we really meant to say
\[
a(n) = a( \lfloor n/3 \rfloor ) + 1
\]

In general the divide-and-conquer relation is a recurrence relation
that looks like this:
\[
a(n) = c_1 \cdot a(n/c_2) + b(n)
\]
where $c_1$ and $c_2$ are constants.

For instance let's consider the time complexity of the binary search.

\begin{console}
def binarysearch(xs, lower, upper, target):
    if lower > upper: return None
    else:
        i = (lower + upper) / 2
        if xs[i] == target:
            return i
        elif xs[i] < target:
            return binarysearch(xs, i + 1, upper, target)
        else:
            return binarysearch(xs, lower, i - 1, target)
\end{console}

If $a(n)$ is the number of steps binary search on an array of size $n$,
then 
\[
a(n) = a(n/2) + c
\]

What about mergesort? The list continually split into two equal parts
to be processed by mergesort. On return from mergesort, the two sublists
are combined. Therefore
\[
b(n) = 2b(n/2) + An + B
\]

Let's compute a closed form for $a(n)$.
We can perform 
\begin{align*}
a(n)
& = a(n/2) + c \\
& = a(n/2^2) + 2c \\
& = a(n/2^3) + 3c \\
& = \cdots \\
& = a(n/2^m) + mc \\
\end{align*}
At some point $n/2^m = 1$ and we reach
\begin{align*}
a(n) = a(1) + mc \\
\end{align*}
From $n / 2^m = 1$ we get
\[
n = 2^m
\]
Hence $m = \log_2 n$.
Note we have
\begin{align*}
a(n)
& = a(1) + c\log_2 n \\
& = A\log_2 n + B
\end{align*}
for some constants $A$ and $B$.
Therefore $a(n) = O(\log n)$.

Hmmm ... I wonder if this can be generalized ...
Well suppose
\[
a(n) = a(n/c_1) + c_2
\]
Doing the same computation I get
\begin{align*}
a(n) 
&= a(n/c_1) + c_2 \\
&= a(n/c_1^2) + 2c_2 \\
&= a(n/c_1^3) + 3c_2 \\
&= \ldots \\
&= a(n/c_1^m) + mc_2 \\
\end{align*}
At some point $n/2^m = 1$ and hence
\[
m = \lg n
\]
Therefore
\begin{align*}
a(n) 
&= a(1) + c_2 \lg n\\
&= A \lg n + B\\
\end{align*}
for constants $A$ and $B$.

Suppose a divide and conquer algorithm looks like this:
\begin{console}
ALGORITHM A
INPUT: Some work w of size n

Let w1,w2,...,wa be subwork of w of sizes n1,n2,...,na

Pre-recursion work
Perform A on w1
Perform A on w2
...
Perform A on wa
Post-recursion work
\end{console}

Let $T(n)$ be the time needed to perform the algorithm on a work of
size $n$.
Suppose the time take for non-recursion work is $f(n)$.
Suppose that the non-recursive parts take $f(n)$ time.
The total time is then
\[
T(n)
= T(n_1) + \cdots + T(n_a) + f(n)
\]
Now I will assume that
\[
n_1 = n_2 = \ldots = n_a = n / b
\]
where $a \geq 1$ and $b > 1$.
(See ??? for divide-and-conquer where the subwork has different sizes.)
The above becomes
\[
T(n)
= a T(n/b) + f(n)
\]
Now I want to know what's the big-O of $T(n)$.
But before that ... here are some examples just to put things into
context.
I'm going to list some runtimes with the corresponding $a$ and $b$.

\begin{eg}
Consider the binary search algorithm on a sorted array.
We have 
\[
T(n) = T(n/2) + A
\]
in the worse case when the target is not found.
Of course when the target is at the middle of the array, the algorithm
terminates right away -- this is of course not the worse case.
So in this case,
\[
a = 1, \,\,\,\,\, b=2, \,\,\,\,\, f(n) = A = \Theta(1)
\]
\qed
\end{eg}

\begin{eg}
Consider the mergesort algorithm on an array.
We have 
\[
T(n) = 2T(n/2) + An + B
\]
So in this case,
\[
a = 2 = b, \,\,\,\,\, f(n) = An + B = \Theta(n)
\]
\qed
\end{eg}


\newpage
Let's play around with the above runtime recurrence
\[
T(n)
= a T(n/b) + f(n)
\]
It's going to be pretty ugly for a general $n$.
Since we continually have to compute $n/b$, let's try it with $n = b^k$.
Then
\begin{align*}
T(b^k)
&= a T \left( b^{k-1} \right) + f(b^k) \\
&= a \left( a T \left( b^{k-2} \right) + f(b^{k-1}) \right)+ f(b^k) \\ 
&= a^2 T \left( b^{k-2} \right) + a f(b^{k-1}) + f(b^k) \\
&= a^2 \left( a T \left( b^{k-3} \right) + f(b^{k-3}) \right) + a f(b^{k-1})
   + f(b^k) \\ 
&= a^3 T \left( b^{k-3} \right) + a^2 f(b^{k-2}) + a f(b^{k-1}) + f(b^k) \\
&= \cdots \\
&= a^k T \left( b^0 \right)
   + \left( a^{k-1} f(b^1) + \cdots + a f(b^{k-1}) + f(b^k) \right)
\end{align*}
Now don't forget that I introduced $k$ just to make $n$ look like $b^k$.
So when I need to, I have to wipe out $k$.
This is easy:
\[
n = b^k \iff \log_b n = k
\]
In other words, later on, I have to replace $k$ by $\log_b n$.

The resulting expression
\[
T(b^k)
= a^k T \left( b^0 \right)
   + \left( a^{k-1} f(b^1) + \cdots + a f(b^{k-1}) + f(b^k) \right)
\]
is a sum of two terms so it's a matter of
which of the two, i.e.,
\[
a^k T \left( b^0 \right)
\]
or
\[
   a^{k-1} f(b^1) + \cdots + a f(b^{k-1}) + f(b^k) 
\]
is \lq\lq bigger'', asymptotically speaking.

We can simplify the first term a little:
\[
a^k T \left( b^0 \right) = \Theta (a^k) = \Theta (a^{\log_b n})
\]
$a^{\log_b n}$ is kind of weird since it's not one of the
standard representative asymptotic functions like $n^\alpha$
or $\log^\alpha$ or $n^\alpha \log^\beta n$ or $c^n$ or etc.
But note that
\[
a^{\log_b n} = n^{\log_b a}
\]
Why? Look at this:
\begin{align*}
(\log_b n)(\log_b a) &= (\log_b a)(\log_b n) \\
\THEREFORE \log_b a^{\log_b n} &= \log_b n^{\log_b a} \\
\THEREFORE a^{\log_b n} &= n^{\log_b a} 
\end{align*}
So the first term is
\[
a^k T \left( b^0 \right)
= \Theta \left( a^{\log_b n} \right)
= \Theta \left( n^{\log_b a} \right)
\]

The second term
\[
   a^{k-1} f(b^1) + \cdots + a f(b^{k-1}) + f(b^k)
\]
looks like a pain.
But ... hmmm it sure looks like a geometric sum.
Of course I can't say anything more. Why? Well ... there's just nothing to say
because I don't have more information about the function $f(n)$.
The sum clearly adds up the amount of work ($k$ pieces of work)
which takes $f(n)$ time 
for a bunch of values of $n$. 
The question is of course: how much work is involved and therefore
how much time, i.e., how big is $f(n)$?
Now from some examples such as binary search, mergesort, etc.
we can have
\[
f(n) = \Theta(1)
\]
(think binary search)
or
\[
f(n) = \Theta(n)
\]
(think mergesort).
So let's say we impose some condition to make $f(n)$ \lq\lq nice'':
\[
f(n) = \Theta(n^c)
\]
say
\[
f(n) = An^c
\]
to be concrete (ignoring unimportant terms in $f(n)$.)
Then I can at least say that 
\begin{align*}
&a^{k-1} f(b^1) + \cdots + a f(b^{k-1}) + f(b^k) \\
&= a^{k-1} \cdot A(b^1)^c
   + \cdots
   + a \cdot A (b^{k-1})^c
   + a \cdot A (b^{k})^c \\
&= A \left(
   a^{k-1} (b^c)^{1}
   \cdots
   + a (b^c)^{k-1}
   + (b^c)^{k}
   \right)
\end{align*}
Well ... too bad ... it's still pretty yucky.
It still looks somewhat like a geometric sum.
We impose the condition $f(n) = \Theta(n^c)$ to make $f(n)$
nice--looking.
Let's make it even nicer.
When I look at the term
\[
a^{k-1} (b^c)^{1}
\]
in the above sum (and also the rest), I see that if $b^c$ is an $a$--power,
I would get a sum of $a$--powers.
So how about I checkout the case when
\[
b^c = a
\]
This means
\[
c = \log_b a
\]
The above becomes
\begin{align*}
&a^{k-1} f(b^1) + \cdots + a f(b^{k-1}) + f(b^k) \\
&= A \left(
   a^{k-1} (b^c)^{1}
   \cdots
   + a (b^c)^{k-1}
   + (b^c)^{k}
   \right)
   \\
&= A \left(
   a^{k-1} (a)^{1}
   \cdots
   + a (a)^{k-1}
   + (a)^{k}
   \right)
   \\
&= A ka^k
\end{align*}
OK, this is not a geometric sum ... but ... at least it's
simplified!
Since $k = \log_b n$,
\begin{align*}
&a^{k-1} f(b^1) + \cdots + a f(b^{k-1}) + f(b^k) \\
&= A ka^k \\
&= A \log_b n \cdot a^{\log_b n} \\
&= \Theta \left( \log_b n \cdot  a^{\log_b n}\right)
\end{align*}
Don't forget that the awkward looking $a^{\log_b n}$ is
the same as $n^{\log_b a}$.
So
\begin{align*}
&a^{k-1} f(b^1) + \cdots + a f(b^{k-1}) + f(b^k) \\
&= \Theta \left( \log_b n \cdot  a^{\log_b n}\right) \\
&= \Theta \left( \log n \cdot  n^{\log_b a}\right)
\end{align*}
(The $\log_b n$ is replaced by $\log n$, since log bases are not
important in big-O or big-$\Theta$.)

Altogether we have the following.
If $n = b^k$, then
\[
T(n) = a^k T \left( b^0 \right)
 + a^{k-1} f(b^1) + \cdots + a f(b^{k-1}) + f(b^k)
\]
where
\[
a^k T \left( b^0 \right) = \Theta \left( n^{\log_b a} \right)
\]
and if $f(n) = \Theta(n^c)$ where $c = \log_b a$, then
\[
a^{k-1} f(b^1) + \cdots + a f(b^{k-1}) + f(b^k) =
\Theta \left( \log n \cdot  n^{\log_b a}\right)
\]
I conclude that 
\[
T(n)
=
\Theta \left( n^{\log_b a} \log n \right)
\]

Before we celebrate, remember that I made the assumption
\[
f(n) = \Theta(n^c), \,\,\,\,\, c = \log_b a
\]
What if
\[
f(n) = \Theta(n^c), \,\,\,\,\, c \neq \log_b a
\]
We're missing the case $c < \log_b a$ and $c > \log_b a$.
Also, what if $f(n) = O(n^c)$?
Or what if $f(n) = O(\log n)$?
Etc.

Here's the famous master theorem ...


\newpage
\begin{thm} \textsc{(Master theorem)}
Let $T : \N \rightarrow \R$ be a function such that
\[
T(n)
= a T \left( \frac{n}{b} \right) + f(n)
\]
where $a \geq 1$ and $b > 1$.
Then
\begin{enumerate}
  \item[(a)] Let $f(n) = O(n^c)$ where $c <  \log_b a$. Then
  \[
  T(n) = \Theta \left( n^{\log_b a} \right)
  \]
  
  \item[(b)] Let $f(n) = \Theta(n^c \log^k n)$ where $c = \log_b a$. Then
  \[
  T(n) = \Theta \left( n^{\log_b a} \log^{k+1} n \right)
  \]

  \item[(c)] Let $f(n) = \Omega$
\end{enumerate}
\end{thm}

\newpage
\begin{eg}
  For binary search
  \[
  a = 1, \,\,\,\,\, b = 2, \,\,\,\,\, f(n) = O(1) = O(n^0), c = 0
  \]
  Note that
  \[
  \log_b a = \log_2 1 = 0
  \]
  and
  \[
  c \not< \log_b a
  \]
  So case 1 of the master theorem does not apply.
\end{eg}

\begin{eg}
  For mergesort search
  \[
  a = 2, \,\,\,\,\, b = 2, \,\,\,\,\, f(n) = \Theta(n) = \Theta(n^c), c = 1
  \]
  Note that
  \[
  \log_b a = \log_2 2 = 1
  \]
  and
  \[
  c = \log_b a
  \]
  So case (b) of Master Theorem applies with $k = 0$.
  This gives us
  \[
  T(n) = \Theta(n \log n)
  \]
  \qed
\end{eg}


\begin{ex}
We have big-O concept for $a(n)$.
Suppose $a(n) = O(b(n))$.
Let
\[
f(x) = \sum_{n = 0}^\infty a(n) x^n
\]
and
\[
g(x) = \sum_{n = 0}^\infty b(n) x^n
\]
What can we say about the relationship between $f(x)$ and $g(x)$ and
vice versa?
Is it true that
\[
a(n) = O(b(n)) \iff f(x) = O(g(x))
\]
\qed
\end{ex}
