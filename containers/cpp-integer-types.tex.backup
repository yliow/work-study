\sectionthree{C++ integer types}
\begin{python0}
from solutions import *; clear()
\end{python0}

You should already know (from CISS240, CISS245) that there
are several integer types.
\begin{myenum}
  \li \verb!int!
  \li \verb!long int!
  \li \verb!long long int!
  \li \verb!unsigned int!
  \li \verb!unsigned long int!
  \li \verb!unsigned long long int!
\end{myenum}
\verb!char! is actually also an integer type,
which is why \verb!char! values can be case labels.
There's also \verb!unsigned char! type.
\verb!bool! is also an integer type.

The problem with the \verb!int!, \verb!long int!,
\verb!long long int! is that the
number of bits used by them is not standardized.
On most machines, \verb!int! is 32 bits, which means an \verb!int!
value lies in the range $-2^{31}, ..., 2^{31} - 1$.
On such systems, the \verb!long long int! is 64 bits
and the \verb!long int! can be either 32 bits or 64 bits.
In the future, an \verb!int! might be 64 bits wide.
(When I was in high school, the PC I had Intel 80286 CPU which has a 16-bit
architecture, i.e., an \verb!int! had 16 bits.)
This can be a problem:
if you bring your \cpp\ code from a machine where an \verb!int!
is 32 bits in length and recompile it on a machine there
the \verb!int! is 16 bits in length,
your program might not work correctly when it processes
an integer such as $2^{16}$.
(Why?)

To make your program portable (i.e., can be compiled and
run on any machine that has a C/C++ compiler),
C/C++ provides integer types with fixed bit lengths.
\begin{myenum}
\li \verb!int8_t!, \verb!uint8_t!
\li \verb!int16_t!, \verb!uint16_t!
\li \verb!int32_t!, \verb!uint32_t!
\li \verb!int64_t!, \verb!uint64_t!
\end{myenum}
The \lq\lq \verb!_t"! in the above means \lq\lq type".
There are constants for the limits of the above types.
For instance for \verb!int32_t!,
the minimum is \verb!INT32_MIN! and
the maximum is \verb!INT32_MAX!.
For \verb!uint32_t!,
the minimum is \verb!0! and
the maximum is \verb!UINT32_MAX!.
Professionally written \cpp\ code tends to use these types.

Another thing to know is that
for containers (including \verb!std::vector!),
we have the concept of the size of a container.
For instance if \verb!v! is a \verb!std::vector< int >! object
the size of \verb!v! is
\[
\texttt{v.size()}
\]
The type of \verb!v.size()! and \verb!v.capacity()!
is \verb!size_t! (it's a \lq\lq size type").
\verb!size_t! is an unsigned integer type and is
either \verb!uint32_t! or \verb!uint64_t!,
depending on the platform.
This makes sense since you don't expect a size to be negative!!!
(It doesn't make sense to say a vector has -5 values.)

When you write something like this:
\begin{console}[fontsize=\footnotesize]
for (int i = 0; i < v.size(); ++i)
{
    ...  
}
\end{console}
you might get a warning since \verb!i! is an \verb!int!
and \verb!v.size()! is an \verb!unsigned int!.
To be absolutely correct, one would write
\begin{console}[fontsize=\footnotesize]
for (unsigned int i = 0; i < v.size(); ++i)
{
    ...  
}
\end{console}
or even better
\begin{console}[fontsize=\footnotesize]
for (size_t i = 0; i < v.size(); ++i)
{
    ...  
}
\end{console}

Of course if you code compute with an integer values which are small,
then it won't make a difference whether you are using \verb!int! values
or \verb!unsigned int! values.
In particular for 32-bit \verb!int!, if you stay in $0,...,2^{31}-1$,
it doesn't matter if you are using \verb!int! or \verb!unsigned int!.
So it's perfectly OK to write
\begin{console}[fontsize=\footnotesize]
for (int i = 0; i < v.size(); ++i)
{
    std::cout << x[i] << '\n';
}
\end{console}
except that you might get compiler warnings.
